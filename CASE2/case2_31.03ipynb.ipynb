{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 - Siemens Advanta\n",
    "\n",
    "Group W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import folium\n",
    "import requests\n",
    "import json\n",
    "from branca.colormap import linear\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import  mean_squared_error, root_mean_squared_error\n",
    "\n",
    "from prophet import Prophet \n",
    "from darts import TimeSeries\n",
    "from darts.models import XGBModel\n",
    "from darts.metrics import rmse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from darts.models.forecasting.xgboost import XGBModel\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#%pip install nixtla\n",
    "from nixtla import NixtlaClient\n",
    "\n",
    "#%pip install neuralforecast\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, NHITS\n",
    "\n",
    "import statsmodels.tsa.stattools as ts\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group started by loading the two data files, `Case2_Market data.xlsx` and `Case2_Sales data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db = pd.read_csv(\"datasets\\Case2_Sales data.csv\", sep=\";\")\n",
    "market_db = pd.read_excel(\"datasets\\Case2_Market data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "### Sales Data\n",
    "The group decided to take a look a `sales_db` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Sales_EUR` was composed of numeric values yet it was stored as an object. There was also an error in `DATE` which is stored as a object instead of a date. These were the first errors to address so that EDA could be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db['Sales_EUR'] = sales_db['Sales_EUR'].str.replace(',', '.').astype(float)\n",
    "sales_db['DATE'] = pd.to_datetime(sales_db['DATE'], format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export data to csv (PowerBI use)\n",
    "# sales_db.to_csv('sales_overview.csv', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the describe method the group found that there was at least one negative value in the `Sales_EUR` column. The group then decided to check for more negative data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales = sales_db[sales_db['Sales_EUR'] < 0]\n",
    "negative_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were more than 200 rows with negative sales values, but were there any underlying patterns that could explain these anomalies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales['DATE'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales['DATE'].dt.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales['DATE'].dt.day.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative values happened at random times, but could they possibly have been connected to a single `Mapped_GCK` (or a group of GCK's)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales['Mapped_GCK'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the dates, it seemed random. The group concluded that there likely wasn't any explanation. Later at the Q&A session with Siemens Advanta the group was informed that these negative values were actually returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the group decided to fix the column names so that only one index existed. This was done by joining the country names at the end (or beginning) of the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = ['DATE',\n",
    "                'Production Index Machinery & Electricals China', 'Shipments Index Machinery & Electricals China',\n",
    "                'Production Index Machinery & Electricals France', 'Shipments Index Machinery & Electricals France',\n",
    "                'Production Index Machinery & Electricals Germany', 'Shipments Index Machinery & Electricals Germany',\n",
    "                'Production Index Machinery & Electricals Italy', 'Shipments Index Machinery & Electricals Italy',\n",
    "                'Production Index Machinery & Electricals Japan', 'Shipments Index Machinery & Electricals Japan',\n",
    "                'Production Index Machinery & Electricals Switzerland', 'Shipments Index Machinery & Electricals Switzerland',\n",
    "                'Production Index Machinery & Electricals United Kingdom', 'Shipments Index Machinery & Electricals United Kingdom',\n",
    "                'Production Index Machinery & Electricals United States', 'Shipments Index Machinery & Electricals United States',\n",
    "                'Production Index Machinery & Electricals Europe', 'Shipments Index Machinery & Electricals Europe',\n",
    "                'World: Price of Base Metals', 'World: Price of Energy', 'World: Price of Metals & Minerals', 'World: Price of Natural gas index', 'World: Price of Crude oil, average', 'World: Price of Copper',\n",
    "                'United States: EUR in LCU',\n",
    "                'Producer Prices United States: Electrical equipment',\n",
    "                'Producer Prices United Kingdom: Electrical equipment',\n",
    "                'Producer Prices Italy: Electrical equipment',\n",
    "                'Producer Prices France: Electrical equipment',\n",
    "                'Producer Prices Germany: Electrical equipment',\n",
    "                'Producer Prices China: Electrical equipment',\n",
    "                'Production Index United States: Machinery and equipment n.e.c.',\n",
    "                'Prodcution Index World: Machinery and equipment n.e.c.',\n",
    "                'Production Index Switzerland: Machinery and equipment n.e.c.',\n",
    "                'Production Index United Kingdom: Machinery and equipment n.e.c.',\n",
    "                'Production Index Italy: Machinery and equipment n.e.c.', \n",
    "                'Production Index  Japan: Machinery and equipment n.e.c.',\n",
    "                'Production Index France: Machinery and equipment n.e.c.',\n",
    "                'Production Index Germany: Machinery and equipment n.e.c.',\n",
    "                'Production Index United States: Electrical equipment',\n",
    "                'Production Index World: Electrical equipment',\n",
    "                'Production Index Switzerland: Electrical equipment',\n",
    "                'Production Index United Kingdom: Electrical equipment',\n",
    "                'Production Index Italy: Electrical equipment',\n",
    "                'Production Index Japan: Electrical equipment',\n",
    "                'Production Index France: Electrical equipment','Production Index Germany: Electrical equipment']\n",
    "\n",
    "market_db.columns = new_col_names\n",
    "market_db.drop(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new names in order the group decided to create a copy of the data where the codes such as \"MAB_ELE_PRO156\" where removed. This was done in a separate dataset because at this point the group had no knowledge of the relevance of these codes. To this point the group had theorized that these were probably codes that represented shortened versions of the column names. The codes were later deemed irrelevant for forecasting by Siemens in the Q&A session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db.drop(1, inplace=True)\n",
    "market_db.reset_index(drop=True, inplace=True)\n",
    "market_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in market_db.columns[1:48]:\n",
    "    market_db[col] = pd.to_numeric(market_db[col], errors='coerce')\n",
    "market_db.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db1 = market_db.copy()\n",
    "market_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db1['DATE'] = pd.to_datetime(market_db1['DATE'].str.strip().str.replace('m', '-'), format='%Y-%m')\n",
    "market_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export data to csv (for PowerBI use)\n",
    "# market_db1.to_csv('market_indexes.csv', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = market_db1.isnull().sum().sort_values(ascending=False)\n",
    "nulls = nulls[nulls > 0]\n",
    "nulls = pd.DataFrame(nulls, columns=['nulls'])\n",
    "nulls['percentage'] = (nulls['nulls'] / len(market_db1) * 100).round(2)\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several columns had missing data, but the ones where the missing data percentage was the highest were:\n",
    "- `Shipments Index Machinery & Electricals United Kingdom` (8%)\n",
    "- `Producer Prices United Kingdom: Electrical equipment` (8%)\n",
    "- `Producer Prices France: Electrical equipment` (15%)\n",
    "- `Producer Prices China: Electrical equipment` (10%)\n",
    "- `Production Index World: Electrical equipment` (5%).\n",
    "\n",
    "The group decided to study these columns further to see wich dates had missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = market_db1[market_db1.isnull().any(axis=1)]['DATE']\n",
    "missing_years = missing_dates.dt.year\n",
    "missing_years.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a smaller portion of the missing rows happened in the time during which the available sales data occurs, as such the group decided to take a closer look at these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates_2018 = market_db1[(market_db1.isnull().any(axis=1)) | (market_db1['DATE'] > '2017-12-31')]['DATE']\n",
    "missing_months = missing_dates_2018.dt.month\n",
    "missing_months.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was at least a week of missing data in every month. Could this be the same week each time? This could possibly be explained by downtime when the server is being maintained or updated, thereby explaining the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates_2018.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there seems to be no particular reason for the missing data, as such it will be addressed in the data cleaning section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db1.describe(). T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers reflect the value of that country's macroeconomic indicator for that month compared to the 2010 baseline:\n",
    "- If the index was higher than 100, that indicator was performing above 2010 baseline\n",
    "- If the index was below 100, was performing below the 2010 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db1[\"United States: EUR in LCU\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"LCU\" means Local Currency Unit and since this column references the United States it indicates a conversion factor. For example, a value of 1.2770 means 1 EUR = 1.2770 USD at that month. \n",
    "\n",
    "Since the sales are in EUR but some macroeconomic indicators like commodity prices are quoted in USD, the group decided to convert them into the same currency by dividing it by the exchange rate. For example, if price of base metals was 50 USD in April 2020 and the exchange rate is 1.10 USD/EUR, then the price in Eur = 50 / 1.10 = 45.45 EUR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_columns = [\n",
    "    'World: Price of Base Metals', \n",
    "    'World: Price of Energy', \n",
    "    'World: Price of Metals & Minerals',\n",
    "    'World: Price of Natural gas index', \n",
    "    'World: Price of Crude oil, average', \n",
    "    'World: Price of Copper',\n",
    "    'Producer Prices United States: Electrical equipment', \n",
    "    'Producer Prices United Kingdom: Electrical equipment',\n",
    "    'Producer Prices Italy: Electrical equipment', \n",
    "    'Producer Prices France: Electrical equipment',\n",
    "    'Producer Prices Germany: Electrical equipment', \n",
    "    'Producer Prices China: Electrical equipment'\n",
    "]\n",
    "\n",
    "for col in price_columns:\n",
    "    market_db1[col] = market_db1[col] / market_db1['United States: EUR in LCU']\n",
    "\n",
    "market_db1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df=market_db1.describe().T\n",
    "describe_df[describe_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production Index Machinery & Electricals_China and Shipments Index Machinery & Electricals_China are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration and Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Data\n",
    "\n",
    "#### General View of the Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db['year'] = sales_db['DATE'].dt.year\n",
    "sales_db['month'] = sales_db['DATE'].dt.month\n",
    "sales_per_month = sales_db.groupby(['year', 'month'])['Sales_EUR'].sum().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(x='month', y='Sales_EUR', hue='year', data=sales_per_month, palette=['#003750', '#2387AA', '#E3700F', '#FFCD50', '#009999'])\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.title('Sales per month (2018-2020, in Millions of EUR)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eda = sales_db.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eda['week_day'] = sales_eda['DATE'].dt.day_name()\n",
    "sales_per_weekday = sales_eda.groupby('week_day')['Sales_EUR'].sum().reset_index()\n",
    "sales_per_weekday['week_day'] = pd.Categorical(sales_per_weekday['week_day'], categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=True)\n",
    "sales_per_weekday = sales_per_weekday.sort_values('week_day')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(x='week_day', y='Sales_EUR', data=sales_per_weekday, palette=['#009999'])\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.title('Sales by Week Day (in Millions of EUR)', fontsize=16)\n",
    "plt.xlabel('Week Day', fontsize=12)\n",
    "plt.ylabel('Sales (EUR)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of sales grouped by weekdays\n",
    "sales_count = sales_eda['week_day'].value_counts(normalize=True).reset_index()\n",
    "sales_count.columns = ['week_day', 'percentage']\n",
    "sales_count['percentage'] *= 100  # Convert to percentage\n",
    "sales_count['week_day'] = pd.Categorical(\n",
    "    sales_count['week_day'], \n",
    "    categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], \n",
    "    ordered=True\n",
    ")\n",
    "sales_count = sales_count.sort_values('week_day')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(x='week_day', y='percentage', data=sales_count, palette=['#009999'])\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.title('Sales Count by Week Day (Percentage)', fontsize=16)\n",
    "plt.xlabel('Week Day', fontsize=12)\n",
    "plt.ylabel('Percentage (%)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sum of sales per month over the span of four years, some underlying patterns were identified.\n",
    "\n",
    "- 2018 is the point where data collection started (in October), meaning it lacks a full year's representation.  \n",
    "- January 2021 starts from a very low revenue point for Siemens (nearing 30 million euros), which could possibly be related to COVID-19.  \n",
    "- 2021 deviates from the trends observed in other years.  \n",
    "- 2022 had only four months of recorded data, showing particularly strong performance compared to other years.\n",
    "- Sales consistently peak in September, which could be attributed to several factors:  \n",
    "  - The end of the fiscal year (often in September or October) leads companies to spend their remaining budget to avoid cuts.  \n",
    "  - Project deadlines drive increased purchasing.  \n",
    "  - Inventory restocking before Q4 to prepare for year-end demand spikes.  \n",
    "  - Seasonal demand fluctuations related to the industry.\n",
    "\n",
    "> Note: This graph is a sum of all values per month in a given year as such it also considers negative values (returns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at how the sales behaved the group decided to see if there was any type of pattern or seasonality in the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales = sales_db[sales_db['Sales_EUR'] < 0]\n",
    "negative_sales['year'] = negative_sales['DATE'].dt.year\n",
    "negative_sales['month'] = negative_sales['DATE'].dt.month\n",
    "sales_per_month = negative_sales.groupby(['year', 'month'])['Sales_EUR'].sum().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(x='month', y='Sales_EUR', hue='year', data=sales_per_month, palette=['#003750', '#2387AA', '#E3700F', '#FFCD50', '#009999'])\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.title('Returns per month (2018-2022, in Millions of EUR)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analysing the returns the group found that:\n",
    "- 2019 (red line) shows relatively stable return rates through most of the year, hovering between -0.1M and -0.2M EUR, with a significant spike in returns around month 10.\n",
    "- 2020 (pink line) shows dramatic improvement over the year, starting with very high returns (around -0.8M EUR) in January but recovering to near 0 by mid-year and maintaining that improvement.\n",
    "- 2021 (purple line) displays a cyclical pattern with notable spikes in returns around months 4, 8, and 11.\n",
    "- 2022 (dark purple line) shows volatility, beginning at -0.2M, improving to near 0 by month 3, declining again around month 4, with partial year data ending around mid-year.\n",
    "- Seasonal patterns appear across years, with months 5-7 typically showing fewer returns, while months 3-4 and 10-11 often show higher return volumes.\n",
    "\n",
    "What about the total value of returns per month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales['month'] = negative_sales['DATE'].dt.month\n",
    "\n",
    "sales_per_month = negative_sales.groupby(['month'])['Sales_EUR'].sum().reset_index()\n",
    "sales_per_month\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 4))\n",
    "sns.set_style('whitegrid')\n",
    "sns.lineplot(x='month', y='Sales_EUR', data=sales_per_month, color='#009999')\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.title('Returns per month (all years, in Millions of EUR)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the sum of returns for all years a couple of observations were made:\n",
    "- January is the overall worst performing month with its loss surpassing 1 million euros.\n",
    "- March and April feature the worst two month run with an overall loss of, just about, 2 million euros.\n",
    "- May to August marks the most positive time of the year where returns are kept at about 200 thousand euro per month\n",
    "- After August returns start to grow, a trend that only seems to stop in Januray (as previously mentioned).\n",
    "\n",
    "The same overall analysis was performed for the general sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db['month'] = sales_db['DATE'].dt.month\n",
    "\n",
    "sales_per_month = sales_db.groupby(['month'])['Sales_EUR'].sum().reset_index()\n",
    "sales_per_month\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "sns.set_style('whitegrid')\n",
    "sns.lineplot(x='month', y='Sales_EUR', data=sales_per_month, color='#009999')\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.title('Sales per month (all years, in Millions of EUR)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres the conclusions the group drew from this plot:\n",
    "- Sales grow the most (and hit their peak) in the first 3 months of the year.\n",
    "- The best selling month is March with over 260 million euros of sales.\n",
    "- The worst performing month is May with just over 180 million euros of sales\n",
    "- Much like returns, sales are at their lowest during the summer suggesting that this is an \"off season\" for these products where neither purchases nor returns are made.\n",
    "- From Augsut to December sales start to rise taking a short fall in November (something that also happens in the returns).\n",
    "\n",
    "> Note: This graph is a sum of all values per month in a given year as such it also considers negative values (returns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating the data into months\n",
    "\n",
    "Since the predictions were to be made on a monthly basis the group decided that it would be better to aggregate the daily sales into monthly sales. This meant that after the aggregation each row would correspond to the total sales value of a month for each GCK. If this value is negative it means that the overall returns on a given month are larger than the sales of that same month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db['year'] = sales_db['DATE'].dt.year\n",
    "sales_db['month'] = sales_db['DATE'].dt.month\n",
    "sales_db = sales_db.groupby(['year', 'month', 'Mapped_GCK'], as_index=False).agg({'Sales_EUR': 'sum', 'DATE': 'first'})\n",
    "sales_db = sales_db[['DATE', 'Mapped_GCK', 'Sales_EUR']]\n",
    "sales_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the monthly data, the group decided to look for any specificities in the mapped GCK's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_gcks = {'#1': '#003750', '#3': '#E3700F', '#5': '#009999'}\n",
    "palette = {gck: highlighted_gcks[gck] if gck in highlighted_gcks else 'gray' for gck in sales_db['Mapped_GCK'].unique()}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sns.set_style('whitegrid')\n",
    "sns.lineplot(x='DATE', y='Sales_EUR', hue='Mapped_GCK', data=sales_db, palette=palette)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "filtered_handles_labels = [(h, l) for h, l in zip(handles, labels) if l in highlighted_gcks]\n",
    "handles, labels = zip(*filtered_handles_labels)\n",
    "ax.legend(handles, labels, title='GCK')\n",
    "\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.title('Sales per GCK (2018-2022, in Millions of EUR)', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Sales (EUR)', fontsize=12)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the line chart above the group concluded that there are 3 GKC's that stand out from the rest. GCK *#3* and *#5* manage to create some distance from the main group (ilustrated in grey) while keeping close to each other, but GCK *#1* represents the product group with the most sales more than doubling the sales of *#3* or *#5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales = sales_db[sales_db['Sales_EUR'] < 0]\n",
    "negative_sales = negative_sales.sort_values('Sales_EUR')\n",
    "negative_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding \"negative sales\" the group found that GCK's *#6*, *#9*, *#14* are the only products where the sales have a negative value. Note that this values are *post-agregation*, meaning these are the only four months where the returns \"out-performed\" the sales.\n",
    "\n",
    "Noting the significant differences in sales scales across GCKs, the group decided to categorize them into low, medium, and high sales. Two of the GCKs (*#9* and *#20*) had to be analyzed separately due to their scale.\n",
    "\n",
    "> Note: These groupings were used solely for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sales = ['#1', '#3', '#5']\n",
    "low_sales = ['#13', '#14', '#36']\n",
    "mid_sales = ['#16', '#11', '#12', '#6', '#8']\n",
    "\n",
    "sales_db['month'] = sales_db['DATE'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_gcks = {'#1': '#003750', '#3': '#E3700F', '#5': '#009999'}\n",
    "group_data = sales_db[sales_db['Mapped_GCK'].isin(high_sales)]\n",
    "monthly_avg = group_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "group_name = 'High Sales'\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for gck in high_sales:\n",
    "    gck_data = group_data[group_data['Mapped_GCK'] == gck]\n",
    "    monthly_avg_gck = gck_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "    sns.lineplot(data=monthly_avg_gck, x='month', y='Sales_EUR', label=f'{gck}', color=highlighted_gcks[gck])\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "plt.title(f'Monthly Average Sales for {group_name} GCKs', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, within the high-sales group, GCK #1 consistently outperformed the others, maintaining average monthly sales above 30 million euros. GCKs #3 and #5 exhibited similar patterns, with both experiencing growth and stabilization in the first trimester, followed by an increase and subsequent decline between August and October. Among them, GCK #5 had the lowest sales, at times barely exceeding 5 million euros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_gcks = {'#16': '#003750', '#11': '#2387AA', '#8': '#E3700F', '#12': '#FFCD50', '#6': '#009999'}\n",
    "group_data = sales_db[sales_db['Mapped_GCK'].isin(mid_sales)]\n",
    "monthly_avg = group_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "group_name = 'Medium Sales'\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for gck in mid_sales:\n",
    "    gck_data = group_data[group_data['Mapped_GCK'] == gck]\n",
    "    monthly_avg_gck = gck_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "    sns.lineplot(data=monthly_avg_gck, x='month', y='Sales_EUR', label=f'{gck}',  color=highlighted_gcks[gck])\n",
    "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x / 1e6:.1f}M'))\n",
    "plt.title(f'Monthly Average Sales for {group_name} GCKs', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The medium-sales group exhibits a wide range, with sales fluctuating from just a few thousand euros to over 3.5 million. GCK #11 is the highest-selling on average, displaying two distinct peaks in March (which also happens for *#16* and *#8*) and September—the latter also observed in GCK #8. All GCKs in this group experienced an uptick in sales during the final trimester of the year. The weakest performer, GCK #12, never surpassed half a million in sales, with its best months occurring in September and November."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_gcks = {'#13': '#003750', '#14': '#E3700F', '#36': '#009999'}\n",
    "group_data = sales_db[sales_db['Mapped_GCK'].isin(low_sales)]\n",
    "monthly_avg = group_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "group_name = 'Low Sales'\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for gck in low_sales:\n",
    "    gck_data = group_data[group_data['Mapped_GCK'] == gck]\n",
    "    monthly_avg_gck = gck_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "    sns.lineplot(data=monthly_avg_gck, x='month', y='Sales_EUR', label=f'{gck}', color=highlighted_gcks[gck])\n",
    "plt.title(f'Monthly Average Sales for {group_name} GCKs', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low-sales group rarely exceeds 70,000 euros, with only GCK #36 reaching this level in two months (April and June). All other values in the group remain below 45,000 euros. GCK #36 exhibits highly volatile sales, in contrast to GCK #13, which maintains relatively stable sales throughout the year, except for a dip in May. GCK #14 experiences its strongest period from April to July, rising from nearly no sales to over 35,000 euros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCKs #9 and #20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = sales_db[sales_db['Mapped_GCK'] == '#9']\n",
    "monthly_avg = group_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "group_name = 'GCK #9'\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.lineplot(data=monthly_avg, x='month', y='Sales_EUR', color = '#009999')\n",
    "plt.title(f'Monthly Average Sales for {group_name}', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCK *#9* shows a sharp spike in March, hitting just over 12,000 euros, before dropping back down in the following months. There's some fluctuation mid-year, but the trend is mostly downward until August. From there, sales climb steadily, peaking again in October at just above 11,000 euros before tapering off in the last two months. A volatile pattern overall, with two clear surges in March and October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = sales_db[sales_db['Mapped_GCK'] == '#20']\n",
    "monthly_avg = group_data.groupby('month')['Sales_EUR'].mean().reset_index()\n",
    "group_name = 'GCK #9'\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.lineplot(data=monthly_avg, x='month', y='Sales_EUR', color = '#009999')\n",
    "plt.title(f'Monthly Average Sales for {group_name}', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Sales (EUR)', fontsize=12)\n",
    "plt.xticks(range(1, 13))\n",
    "sns.despine(bottom=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCK *#20* has a massive spike in April, reaching over 4,500 euros, but immediately crashes in May, barely clearing 500 euros. Before that, sales are mostly flat, staying under 1,000 euros. After the drop, there's some minor fluctuation mid-year, but nothing substantial. Then, another surge in October, peaking just above 3,000 euros, followed by a gradual decline. A pattern of two big spikes with long periods of low activity in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Conclusions\n",
    "\n",
    "The group was able to conclude that on average:\n",
    "\n",
    "- There are two peaks during the year happening just before summer season and right afterwards\n",
    "- During the summer sales seem to drop and stabilize\n",
    "- In some cases sales seem to rise during the last trimester, although this seems to be a trend that mainly affects the medium sales group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db.drop(columns=['month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for characteritics and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into the modelling phase, the group decomposed the time series to extract the Seasonality, Trends, Autocorrelation and Stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_product = {}\n",
    "mapped_gck_list = sales_db['Mapped_GCK'].unique().tolist()\n",
    "for i in mapped_gck_list:\n",
    "    sales_product[i] = sales_db[sales_db[\"Mapped_GCK\"] == i].set_index(\"DATE\")\n",
    "    sales_product[i].drop(columns=[\"Mapped_GCK\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seasonality and Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=['#009999'])\n",
    "for i in mapped_gck_list:\n",
    "    result = seasonal_decompose(sales_product[i], model='additive', period=12)\n",
    "    fig = result.plot()\n",
    "    fig.set_size_inches(7, 4)\n",
    "    plt.suptitle(f'Seasonal Decomposition for GCK {i}', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding trends the product groups were distributed as follows:\n",
    "- Uptrend: #11; #12; #36; #8\n",
    "- Horizontal Trend: #1; #14; #20; #3; #4; #5; #9\n",
    "- Downtrend: #13; #16; #6\n",
    "\n",
    "The majority of graphics appeared to have some type of seasonality (mostly visble from the third image onwards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autocorrelation (ACF and PACF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mapped_gck_list:\n",
    "    fig = plot_acf(sales_product[i], title=f'Autocorrelation for GCK {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data did not seem to be autocorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PACF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mapped_gck_list:\n",
    "    fig = plot_pacf(sales_product[i], title=f'Autocorrelation for GCK {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis of the ACF and PAFC plots, most of the product groups do not show meaningful correlations. This means that past values only have a minor influence on future values. Adding external data (market_db) could possibly enhance the predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stationarity (Dickey-Fuller test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mapped_gck_list:\n",
    "    adf, pval, usedlag, nobs, crit_vals, icbest = adfuller(sales_product[i][\"Sales_EUR\"].values)\n",
    "    print(f'GCK {i}')\n",
    "    print('ADF test statistic:', adf)\n",
    "    print('ADF p-value:', pval)\n",
    "    print('Number of lags used:', usedlag)\n",
    "    print('Number of observations:', nobs)\n",
    "    print('Critical values:', crit_vals)\n",
    "    print('Best information criterion:', icbest, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All product groups except one were stationary. Product group #8 isn't and therefore it will need to undergo a transformation (differencing) to become stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_index_machinery_eletricals = ['Production Index Machinery & Electricals China', \n",
    "                                         'Production Index Machinery & Electricals France',\n",
    "                                         'Production Index Machinery & Electricals Germany', \n",
    "                                         'Production Index Machinery & Electricals Italy',\n",
    "                                         'Production Index Machinery & Electricals Japan',\n",
    "                                         'Production Index Machinery & Electricals Switzerland',\n",
    "                                         'Production Index Machinery & Electricals United Kingdom', \n",
    "                                         'Production Index Machinery & Electricals United States',\n",
    "                                         'Production Index Machinery & Electricals Europe']\n",
    "                    \n",
    "production_index_machinery = ['Production Index United States: Machinery and equipment n.e.c.',\n",
    "                            'Prodcution Index World: Machinery and equipment n.e.c.',\n",
    "                            'Production Index Switzerland: Machinery and equipment n.e.c.',\n",
    "                            'Production Index United Kingdom: Machinery and equipment n.e.c.',\n",
    "                            'Production Index Italy: Machinery and equipment n.e.c.',\n",
    "                            'Production Index  Japan: Machinery and equipment n.e.c.',\n",
    "                            'Production Index France: Machinery and equipment n.e.c.',\n",
    "                            'Production Index Germany: Machinery and equipment n.e.c.']\n",
    "\n",
    "production_index_eletrical = ['Production Index United States: Electrical equipment',\n",
    "                            'Production Index World: Electrical equipment',\n",
    "                            'Production Index Switzerland: Electrical equipment',\n",
    "                            'Production Index United Kingdom: Electrical equipment',\n",
    "                            'Production Index Italy: Electrical equipment',\n",
    "                            'Production Index Japan: Electrical equipment',\n",
    "                            'Production Index France: Electrical equipment',\n",
    "                            'Production Index Germany: Electrical equipment']\n",
    "\n",
    "shipment_index = ['Shipments Index Machinery & Electricals China',\n",
    "                  'Shipments Index Machinery & Electricals France',\n",
    "                  'Shipments Index Machinery & Electricals Germany',\n",
    "                  'Shipments Index Machinery & Electricals Italy',\n",
    "                  'Shipments Index Machinery & Electricals Japan',\n",
    "                  'Shipments Index Machinery & Electricals Switzerland',\n",
    "                  'Shipments Index Machinery & Electricals United Kingdom',\n",
    "                  'Shipments Index Machinery & Electricals Europe']\n",
    "\n",
    "world_price = [col for col in market_db1.columns if 'World: Price' in col]\n",
    "\n",
    "producer_prices = ['Producer Prices United States: Electrical equipment',\n",
    "                   'Producer Prices United Kingdom: Electrical equipment',\n",
    "                   'Producer Prices Italy: Electrical equipment',\n",
    "                   'Producer Prices France: Electrical equipment',\n",
    "                   'Producer Prices Germany: Electrical equipment',\n",
    "                   'Producer Prices China: Electrical equipment']\n",
    "\n",
    "def plot_indicators(df, indicator_list, title):\n",
    "    filtered_data = df[df['Indicator'].isin(indicator_list)]\n",
    "    fig = px.line(filtered_data, x=\"DATE\", y=\"Value\", color=\"Indicator\", title=title,\n",
    "                  labels={\"Value\": \"Cost in EUR\" if \"Price\" in title else \"Index Value\", \"DATE\": \"Date\", \"Indicator\": \"Indicator\"})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_melted = market_db1.melt(id_vars=[\"DATE\"], var_name=\"Indicator\", value_name=\"Value\")\n",
    "plot_indicators(market_melted, production_index_machinery_eletricals, \"Production Index Trends Over Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group observed that during the 2008–2009 financial crisis, most major economies saw a drop in their production indices relative to 2010 (baseline = 100). China, however, maintained higher production levels thanks to strong domestic demand and substantial government stimulus. Because the indices compare each month to its own 2010 baseline, China’s continued growth after 2010 results in a less pronounced dip compared to the sharper declines seen in other countries’ production levels. In other words, while most economies saw a steep production decline relative to 2010, China’s production levels, even during the crisis, stayed closer to or above its 2010 benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(market_melted[market_melted['Indicator'].isin(production_index_machinery_eletricals)], \n",
    "                  col=\"Indicator\", col_wrap=3, sharey=False, height=4, aspect=2)\n",
    "g.map(sns.lineplot, \"DATE\", \"Value\", color='#009999')\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Date\", \"Value\")\n",
    "g.fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_db = {}\n",
    "\n",
    "for col in production_index_machinery_eletricals:\n",
    "    country = \" \".join(col.split()[5:])\n",
    "    map_db[country] = map_db.get(country, {})\n",
    "    map_db[country]['Production Index Machinery & Electricals'] = market_db1[col].mean()\n",
    "\n",
    "map_db['United States of America'] = map_db.pop('United States')\n",
    "del map_db['Europe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map_db\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index').reset_index()\n",
    "df.columns = ['Country', 'Production Index Machinery & Electricals']\n",
    "\n",
    "world_geo = requests.get(\n",
    "    \"https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json\"\n",
    ").json()\n",
    "\n",
    "m = folium.Map(location=[20, 0], zoom_start=2, tiles=\"cartodb positron\")\n",
    "\n",
    "colormap = linear.Reds_09.scale(\n",
    "    df['Production Index Machinery & Electricals'].min(),\n",
    "    df['Production Index Machinery & Electricals'].max()\n",
    ")\n",
    "\n",
    "choropleth = folium.Choropleth(\n",
    "    geo_data=world_geo,\n",
    "    name=\"choropleth\",\n",
    "    data=df,\n",
    "    columns=[\"Country\", 'Production Index Machinery & Electricals'],\n",
    "    key_on=\"feature.properties.name\",\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    highlight=True,\n",
    "    line_color='black',\n",
    "    legend_name=\"Production Index Machine\",\n",
    ").add_to(m)\n",
    "\n",
    "df['color'] = df['Production Index Machinery & Electricals'].apply(colormap)\n",
    "\n",
    "style_function = lambda x: {\n",
    "    'fillColor': '#f0f0f0',\n",
    "    'color': '#d0d0d0',\n",
    "    'fillOpacity': 0.2,\n",
    "    'weight': 0.5\n",
    "}\n",
    "folium.GeoJson(\n",
    "    world_geo,\n",
    "    name=\"World Countries\",\n",
    "    style_function=style_function,\n",
    "    control=False\n",
    ").add_to(m)\n",
    "\n",
    "choropleth.add_to(m)\n",
    "\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['name'], labels=False)\n",
    ")\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indicators(market_melted, production_index_machinery, \"Production Index Machinery Trends Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(market_melted[market_melted['Indicator'].isin(production_index_machinery)], \n",
    "                  col=\"Indicator\", col_wrap=3, sharey=False, height=4, aspect=2)\n",
    "g.map(sns.lineplot, \"DATE\", \"Value\", color='#009999')\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Date\", \"Value\")\n",
    "g.fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_db = {}\n",
    "for col in production_index_machinery:\n",
    "    country = ' '.join(col.split()[2:4])\n",
    "    map_db[country] = map_db.get(country, {})\n",
    "    map_db[country]['Machinery and equipment'] = market_db1[col].mean()\n",
    "\n",
    "map_db['United States of America'] = map_db.pop('United States:')\n",
    "map_db['United Kingdom'] = map_db.pop('United Kingdom:')\n",
    "map_db['Switzerland'] = map_db.pop('Switzerland: Machinery')\n",
    "map_db['Italy'] = map_db.pop('Italy: Machinery')\n",
    "map_db['Japan'] = map_db.pop('Japan: Machinery')\n",
    "map_db['France'] = map_db.pop('France: Machinery')\n",
    "map_db[ 'Germany'] = map_db.pop( 'Germany: Machinery')\n",
    "del map_db['World: Machinery']\n",
    "\n",
    "map_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map_db\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index').reset_index()\n",
    "df.columns = ['Country', 'Machinery and equipment']\n",
    "\n",
    "world_geo = requests.get(\n",
    "    \"https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json\"\n",
    ").json()\n",
    "\n",
    "m = folium.Map(location=[20, 0], zoom_start=2, tiles=\"cartodb positron\")\n",
    "\n",
    "colormap = linear.Reds_09.scale(\n",
    "    df['Machinery and equipment'].min(),\n",
    "    df['Machinery and equipment'].max()\n",
    ")\n",
    "\n",
    "choropleth = folium.Choropleth(\n",
    "    geo_data=world_geo,\n",
    "    name=\"choropleth\",\n",
    "    data=df,\n",
    "    columns=[\"Country\", 'Machinery and equipment'],\n",
    "    key_on=\"feature.properties.name\",\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    highlight=True,\n",
    "    line_color='black',\n",
    "    legend_name=\"Production Index Machine\",\n",
    ").add_to(m)\n",
    "\n",
    "df['color'] = df['Machinery and equipment'].apply(colormap)\n",
    "\n",
    "style_function = lambda x: {\n",
    "    'fillColor': '#f0f0f0', \n",
    "    'color': '#d0d0d0',   \n",
    "    'fillOpacity': 0.2,\n",
    "    'weight': 0.5\n",
    "}\n",
    "\n",
    "\n",
    "folium.GeoJson(\n",
    "    world_geo,\n",
    "    name=\"World Countries\",\n",
    "    style_function=style_function,\n",
    "    control=False \n",
    ").add_to(m)\n",
    "\n",
    "choropleth.add_to(m)\n",
    "\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['name'], labels=False)\n",
    ")\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indicators(market_melted, production_index_eletrical, \"Production Index Electrical Equipment Trends Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(market_melted[market_melted['Indicator'].isin(production_index_eletrical)], \n",
    "                  col=\"Indicator\", col_wrap=3, sharey=False, height=4, aspect=2)\n",
    "g.map(sns.lineplot, \"DATE\", \"Value\", color='#009999')\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Date\", \"Value\")\n",
    "g.fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_db = {}\n",
    "\n",
    "for col in production_index_machinery:\n",
    "    country = ' '.join(col.split()[2:4])  \n",
    "    map_db[country] = map_db.get(country, {})\n",
    "    map_db[country]['Electrical equipment'] = market_db1[col].mean()\n",
    "\n",
    "map_db['United States of America'] = map_db.pop('United States:')\n",
    "map_db['United Kingdom'] = map_db.pop('United Kingdom:')\n",
    "map_db['Switzerland'] = map_db.pop('Switzerland: Machinery')\n",
    "map_db['Italy'] = map_db.pop('Italy: Machinery')\n",
    "map_db['Japan'] = map_db.pop('Japan: Machinery')\n",
    "map_db['France'] = map_db.pop('France: Machinery')\n",
    "map_db[ 'Germany'] = map_db.pop( 'Germany: Machinery')\n",
    "del map_db['World: Machinery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = map_db\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index').reset_index()\n",
    "df.columns = ['Country', 'Electrical equipment']\n",
    "\n",
    "world_geo = requests.get(\n",
    "    \"https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json\"\n",
    ").json()\n",
    "\n",
    "m = folium.Map(location=[20, 0], zoom_start=2, tiles=\"cartodb positron\")\n",
    "\n",
    "colormap = linear.Reds_09.scale(\n",
    "    df['Electrical equipment'].min(),\n",
    "    df['Electrical equipment'].max()\n",
    ")\n",
    "\n",
    "choropleth = folium.Choropleth(\n",
    "    geo_data=world_geo,\n",
    "    name=\"choropleth\",\n",
    "    data=df,\n",
    "    columns=[\"Country\", 'Electrical equipment'],\n",
    "    key_on=\"feature.properties.name\",\n",
    "    fill_color='YlOrRd',  \n",
    "    line_opacity=0.2,\n",
    "    highlight=True,\n",
    "    line_color='black',\n",
    "    legend_name=\"Electrical equipment\",\n",
    ").add_to(m)\n",
    "\n",
    "df['color'] = df['Electrical equipment'].apply(colormap)\n",
    "\n",
    "style_function = lambda x: {\n",
    "    'fillColor': '#f0f0f0',  \n",
    "    'color': '#d0d0d0',    \n",
    "    'fillOpacity': 0.2,\n",
    "    'weight': 0.5\n",
    "}\n",
    "\n",
    "\n",
    "folium.GeoJson(\n",
    "    world_geo,\n",
    "    name=\"World Countries\",\n",
    "    style_function=style_function,\n",
    "    control=False  \n",
    ").add_to(m)\n",
    "\n",
    "choropleth.add_to(m)\n",
    "\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['name'], labels=False)\n",
    ")\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indicators(market_melted, shipment_index, \"Shipments Index Trends Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(market_melted[market_melted['Indicator'].isin(shipment_index)], \n",
    "                  col=\"Indicator\", col_wrap=3, sharey=False, height=4, aspect=2)\n",
    "g.map(sns.lineplot, \"DATE\", \"Value\", color='#009999')\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Date\", \"Value\")\n",
    "g.fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the Shipments, Germany, Italy, Japan, Switzerland, the UK, and Europe as a whole showed a sharp decline in shipments during 2008–2009. This drop reflects weaker global demand and disruptions in trade. China’s shipments keep climbing. Nearly all plots show a dip around early 2020, coinciding with lockdowns, supply chain disruptions, and reduced industrial activity. By late 2020 and into 2021, shipments rebound as economies reopen, although the pace varies by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_db = {}\n",
    "\n",
    "for col in shipment_index:\n",
    "    country = ' '.join(col.split()[5:])  \n",
    "    map_db[country] = map_db.get(country, {})\n",
    "    map_db[country]['Electrical equipment'] = market_db1[col].mean()\n",
    "\n",
    "del map_db['Europe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = map_db\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index').reset_index()\n",
    "df.columns = ['Country', 'Shipments Index Machinery & Electricals']\n",
    "\n",
    "\n",
    "world_geo = requests.get(\n",
    "    \"https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json\"\n",
    ").json()\n",
    "\n",
    "m = folium.Map(location=[20, 0], zoom_start=2, tiles=\"cartodb positron\")\n",
    "\n",
    "# Create a custom color scale that uses reds instead of blues\n",
    "colormap = linear.Reds_09.scale(\n",
    "    df['Shipments Index Machinery & Electricals'].min(),\n",
    "    df['Shipments Index Machinery & Electricals'].max()\n",
    ")\n",
    "\n",
    "# Add choropleth layer\n",
    "choropleth = folium.Choropleth(\n",
    "    geo_data=world_geo,\n",
    "    name=\"choropleth\",\n",
    "    data=df,\n",
    "    columns=[\"Country\", 'Shipments Index Machinery & Electricals'],\n",
    "    key_on=\"feature.properties.name\",\n",
    "    fill_color='YlOrRd',  # Use a predefined color scheme\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    highlight=True,\n",
    "    line_color='black',\n",
    "    legend_name=\"Shipments Index Machinery & Electricals\",\n",
    ").add_to(m)\n",
    "\n",
    "df['color'] = df['Shipments Index Machinery & Electricals'].apply(colormap)\n",
    "\n",
    "style_function = lambda x: {\n",
    "    'fillColor': '#f0f0f0',  \n",
    "    'color': '#d0d0d0',      \n",
    "    'fillOpacity': 0.2,\n",
    "    'weight': 0.5\n",
    "}\n",
    "\n",
    "folium.GeoJson(\n",
    "    world_geo,\n",
    "    name=\"World Countries\",\n",
    "    style_function=style_function,\n",
    "    control=False \n",
    ").add_to(m)\n",
    "\n",
    "choropleth.add_to(m)\n",
    "\n",
    "\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['name'], labels=False)\n",
    ")\n",
    "\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indicators(market_melted, world_price, \"World Price Trends Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indicators(market_melted, producer_prices, \"Producer Prices Trends Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_db = {}\n",
    "\n",
    "for col in producer_prices:\n",
    "    country = ' '.join(col.split()[2:4])  \n",
    "    map_db[country] = map_db.get(country, {})\n",
    "    map_db[country]['Producer Prices'] = market_db1[col].mean()\n",
    "\n",
    "map_db['United States of America'] = map_db.pop('United States:')\n",
    "map_db['United Kingdom'] = map_db.pop('United Kingdom:')\n",
    "map_db['Italy'] = map_db.pop('Italy: Electrical')\n",
    "map_db['France'] = map_db.pop('France: Electrical')\n",
    "map_db[ 'Germany'] = map_db.pop( 'Germany: Electrical')\n",
    "map_db['China'] = map_db.pop('China: Electrical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = map_db\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index').reset_index()\n",
    "df.columns = ['Country', 'Producer Prices']\n",
    "\n",
    "\n",
    "world_geo = requests.get(\n",
    "    \"https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json\"\n",
    ").json()\n",
    "\n",
    "\n",
    "m = folium.Map(location=[20, 0], zoom_start=2, tiles=\"cartodb positron\")\n",
    "\n",
    "colormap = linear.Reds_09.scale(\n",
    "    df['Producer Prices'].min(),\n",
    "    df['Producer Prices'].max()\n",
    ")\n",
    "\n",
    "\n",
    "choropleth = folium.Choropleth(\n",
    "    geo_data=world_geo,\n",
    "    name=\"choropleth\",\n",
    "    data=df,\n",
    "    columns=[\"Country\", 'Producer Prices'],\n",
    "    key_on=\"feature.properties.name\",\n",
    "    fill_color='YlOrRd',  \n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    highlight=True,\n",
    "    line_color='black',\n",
    "    legend_name=\"Producer Prices\",\n",
    ").add_to(m)\n",
    "\n",
    "\n",
    "df['color'] = df['Producer Prices'].apply(colormap)\n",
    "\n",
    "\n",
    "style_function = lambda x: {\n",
    "    'fillColor': '#f0f0f0',\n",
    "    'color': '#d0d0d0',      \n",
    "    'fillOpacity': 0.2,\n",
    "    'weight': 0.5\n",
    "}\n",
    "\n",
    "\n",
    "folium.GeoJson(\n",
    "    world_geo,\n",
    "    name=\"World Countries\",\n",
    "    style_function=style_function,\n",
    "    control=False  \n",
    ").add_to(m)\n",
    "\n",
    "choropleth.add_to(m)\n",
    "\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['name'], labels=False)\n",
    ")\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots about the commodity prices and producer prices the group observed some trends:\n",
    "- 2008-2009 financial crisis: Around 2008, the price of the commodities (e.g. metals, energy) reached a peak and then fell as the global crisis unfolded. During this period, the industrial activity worldwide slowed down, reducing the demand for raw materials and leading to a drop in prices. Although commodity prices fell, producer prices sometimes show a lag or a different pace of change. In some regions or sectors, producer prices may have initially increased or remained elevated before eventually reflecting the downturn in input costs.\n",
    "\n",
    "- Post-Crisis Recovery (2010-2011): After 2010/2011, both commodity and producer prices started to increase as the global economic activity started to improve. Emerging markets, in particular, fueled demand for raw materials, contributing to the recovery.\n",
    "\n",
    "- 2014-2017: While producer prices continued an upward trend, reaching a peak around January 2017, some commodities experienced oversupply or softer demand, causing their prices to plateau or decrease.\n",
    "\n",
    "- Covid-19 Impact (2020): Early 2020, lockdowns and reduced industrial output caused a big drop in many commodity and producer prices, reflecting the sudden contraction in economic activity. As economies reopened in 2021, pent-up demand and supply chain disruptions led to price increases. Commodities rebounded, and producer prices also surged in response to higher input costs and renewed consumer demand.\n",
    "\n",
    "- 2021-2022 price surge: The price increases from 2021 to 2022 were the largest rise in the past 20 years, partly driven by disruptions in shipping, logistics, and labor markets, rising costs (e.g., energy, metals) feeding into producer prices and, ultimately, consumer prices (inflation). Also, as industries ramped up production, demand outpaced supply, further fueling price hikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for characteristics and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Sales Data, the Market Data was also decomposed to check to the Seasonality, Trends, Autocorrelation and Stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seasonality and Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_copy = market_db1.copy()\n",
    "market_db1 = market_db1.set_index(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = market_db1.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in production_index_machinery_eletricals:\n",
    "    market_indicator = market_db1[i].to_frame()\n",
    "    market_indicator = market_indicator.fillna(method='ffill').fillna(method='bfill')\n",
    "    result = seasonal_decompose(market_indicator[i], model='additive', period=12)\n",
    "    result.plot()\n",
    "    plt.suptitle(f'Seasonal Decomposition for {i}', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the trends in the production of machinery and eletricals, the product groups are distributed as follows:\n",
    "- Uptrend: China\n",
    "- Horizontal: France, Germany, Italy, Japan, Switzerland, United Kingdom, United Sates of America; Europe\n",
    "- Downtredn: NA\n",
    "\n",
    "In all indexes there is presence of seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in production_index_machinery:\n",
    "    market_indicator = market_db1[i].to_frame()\n",
    "    market_indicator = market_indicator.fillna(method='ffill').fillna(method='bfill')\n",
    "    result = seasonal_decompose(market_indicator[i], model='additive', period=12)\n",
    "    result.plot()\n",
    "    plt.suptitle(f'Seasonal Decomposition for {i}', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the trends in the production of machinery and eletricals, the product groups are distributed as follows:\n",
    "- Uptrend: NA\n",
    "- Horizontal: United Sates of America; World; Switzerland; United Kingdom; Italy; Japan; France; Germany\n",
    "- Downtredn: NA\n",
    "\n",
    "In all indexes there is presence of seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in shipment_index:\n",
    "    market_indicator = market_db1[i].to_frame()\n",
    "    market_indicator = market_indicator.fillna(method='ffill').fillna(method='bfill')\n",
    "    result = seasonal_decompose(market_indicator[i], model='additive', period=12)\n",
    "    result.plot()\n",
    "    plt.suptitle(f'Seasonal Decomposition for {i}', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the trends in the production of machinery and eletricals, the product groups are distributed as follows:\n",
    "- Uptrend: China; Germany; Europe\n",
    "- Horizontal: Switzerland; United Kingdom; Italy; Japan; France\n",
    "- Downtredn: NA\n",
    "\n",
    "In all indexes there is presence of seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in world_price:\n",
    "    market_indicator = market_db1[i].to_frame()\n",
    "    market_indicator = market_indicator.fillna(method='ffill').fillna(method='bfill')\n",
    "    result = seasonal_decompose(market_indicator[i], model='additive', period=12)\n",
    "    result.plot()\n",
    "    plt.suptitle(f'Seasonal Decomposition for {i}', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the world prices, the indexes trending as follwos:\n",
    "- Uptrend: Copper\n",
    "- Horizontal: Base Metals; Price Energy; Metals & Minerals; Gas; Crude Oil\n",
    "- Downtrend: NA\n",
    "\n",
    "Again, all indexes show signs of seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autocorrelation (ACF)\n",
    "\n",
    "The group opted to comment this section to keep the notebook simpler and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in indicators:\n",
    "#     market_indicator = market_db1[[i]].fillna(method='ffill').fillna(method='bfill')\n",
    "#     fig = plot_acf(market_indicator[i], title=f'Autocorrelation for Indicator {i}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stationarity (Dickey-Fuller test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db1_filtered = market_db1[market_db1.index >= '2018-10']\n",
    "\n",
    "for i in production_index_eletrical:\n",
    "    market_indicator = market_db1_filtered[[i]].fillna(method='ffill').fillna(method='bfill')\n",
    "    adf, pval, usedlag, nobs, crit_vals, icbest = adfuller(market_indicator[i].values)\n",
    "    \n",
    "    print(f'Indicator: {i}')\n",
    "    print('ADF test statistic:', adf)\n",
    "    print('ADF p-value:', pval)\n",
    "    print('Number of lags used:', usedlag)\n",
    "    print('Number of observations:', nobs)\n",
    "    print('Critical values:', crit_vals)\n",
    "    print('Best information criterion:', icbest)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in shipment_index:\n",
    "    market_indicator = market_db1_filtered[[i]].fillna(method='ffill').fillna(method='bfill')\n",
    "    adf, pval, usedlag, nobs, crit_vals, icbest = adfuller(market_indicator[i].values)\n",
    "    \n",
    "    print(f'Indicator: {i}')\n",
    "    print('ADF test statistic:', adf)\n",
    "    print('ADF p-value:', pval)\n",
    "    print('Number of lags used:', usedlag)\n",
    "    print('Number of observations:', nobs)\n",
    "    print('Critical values:', crit_vals)\n",
    "    print('Best information criterion:', icbest)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in world_price:\n",
    "    market_indicator = market_db1_filtered[[i]].fillna(method='ffill').fillna(method='bfill')\n",
    "    adf, pval, usedlag, nobs, crit_vals, icbest = adfuller(market_indicator[i].values)\n",
    "    \n",
    "    print(f'Indicator: {i}')\n",
    "    print('ADF test statistic:', adf)\n",
    "    print('ADF p-value:', pval)\n",
    "    print('Number of lags used:', usedlag)\n",
    "    print('Number of observations:', nobs)\n",
    "    print('Critical values:', crit_vals)\n",
    "    print('Best information criterion:', icbest)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in producer_prices:\n",
    "    market_indicator = market_db1_filtered[[i]].fillna(method='ffill').fillna(method='bfill')\n",
    "    adf, pval, usedlag, nobs, crit_vals, icbest = adfuller(market_indicator[i].values)\n",
    "    \n",
    "    print(f'Indicator: {i}')\n",
    "    print('ADF test statistic:', adf)\n",
    "    print('ADF p-value:', pval)\n",
    "    print('Number of lags used:', usedlag)\n",
    "    print('Number of observations:', nobs)\n",
    "    print('Critical values:', crit_vals)\n",
    "    print('Best information criterion:', icbest)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of your macroeconomic indicators show non-stationarity (p-value > 0.05), they were transformed to achieve stationarity before being used in the model.\n",
    "- Production Index United States: Electrical equipment\n",
    "- Production Index World: Electrical equipment\n",
    "- Production Index Switzerland: Electrical equipment\n",
    "- Production Index Italy: Electrical equipment\n",
    "- Production Index Japan: Electrical equipment\n",
    "- Production Index Germany: Electrical equipment\n",
    "- Shipments Index Machinery & Electricals France\n",
    "- Shipments Index Machinery & Electricals Germany\n",
    "- Shipments Index Machinery & Electricals Japan\n",
    "- Shipments Index Machinery & Electricals Switzerland\n",
    "- Shipments Index Machinery & Electricals Europe\n",
    "- World: Price of Base Metals\n",
    "- World: Price of Energy\n",
    "- World: Price of Metals & Minerals\n",
    "- World: Price of Natural gas index\n",
    "- World: Price of Crude oil, average\n",
    "- World: Price of Copper\n",
    "- Producer Prices United States: Electrical equipment\n",
    "- Producer Prices United Kingdom: Electrical equipment\n",
    "- Producer Prices Italy: Electrical equipment\n",
    "- Producer Prices France: Electrical equipment\n",
    "- Producer Prices Germany: Electrical equipment\n",
    "- Producer Prices China: Electrical equipment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Sales Data with Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db = pd.read_csv(\"datasets\\Case2_Sales data.csv\", sep=\";\")\n",
    "market_db = pd.read_excel(\"datasets\\Case2_Market data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the group corrected the datatypes and converted daily to monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db['Sales_EUR'] = sales_db['Sales_EUR'].str.replace(',', '.').astype(float)\n",
    "sales_db.columns = sales_db.columns.str.strip()\n",
    "sales_db['DATE'] = pd.to_datetime(sales_db['DATE'], dayfirst=True)\n",
    "sales_db['DATE'] = sales_db['DATE'].dt.to_period('M')\n",
    "sales_db = sales_db.groupby(['DATE', 'Mapped_GCK'])['Sales_EUR'].sum().reset_index()\n",
    "sales_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the market data, the group improved the structure of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = ['DATE',\n",
    "                'Production Index Machinery & Electricals China', 'Shipments Index Machinery & Electricals China',\n",
    "                'Production Index Machinery & Electricals France', 'Shipments Index Machinery & Electricals France',\n",
    "                'Production Index Machinery & Electricals Germany', 'Shipments Index Machinery & Electricals Germany',\n",
    "                'Production Index Machinery & Electricals Italy', 'Shipments Index Machinery & Electricals Italy',\n",
    "                'Production Index Machinery & Electricals Japan', 'Shipments Index Machinery & Electricals Japan',\n",
    "                'Production Index Machinery & Electricals Switzerland', 'Shipments Index Machinery & Electricals Switzerland',\n",
    "                'Production Index Machinery & Electricals United Kingdom', 'Shipments Index Machinery & Electricals United Kingdom',\n",
    "                'Production Index Machinery & Electricals United States', 'Shipments Index Machinery & Electricals United States',\n",
    "                'Production Index Machinery & Electricals Europe', 'Shipments Index Machinery & Electricals Europe',\n",
    "                'World: Price of Base Metals', 'World: Price of Energy', 'World: Price of Metals & Minerals', 'World: Price of Natural gas index', 'World: Price of Crude oil, average', 'World: Price of Copper',\n",
    "                'United States: EUR in LCU',\n",
    "                'Producer Prices United States: Electrical equipment',\n",
    "                'Producer Prices United Kingdom: Electrical equipment',\n",
    "                'Producer Prices Italy: Electrical equipment',\n",
    "                'Producer Prices France: Electrical equipment',\n",
    "                'Producer Prices Germany: Electrical equipment',\n",
    "                'Producer Prices China: Electrical equipment',\n",
    "                'Production Index United States: Machinery and equipment n.e.c.',\n",
    "                'Prodcution Index World: Machinery and equipment n.e.c.',\n",
    "                'Production Index Switzerland: Machinery and equipment n.e.c.',\n",
    "                'Production Index United Kingdom: Machinery and equipment n.e.c.',\n",
    "                'Production Index Italy: Machinery and equipment n.e.c.', \n",
    "                'Production Index  Japan: Machinery and equipment n.e.c.',\n",
    "                'Production Index France: Machinery and equipment n.e.c.',\n",
    "                'Production Index Germany: Machinery and equipment n.e.c.',\n",
    "                'Production Index United States: Electrical equipment',\n",
    "                'Production Index World: Electrical equipment',\n",
    "                'Production Index Switzerland: Electrical equipment',\n",
    "                'Production Index United Kingdom: Electrical equipment',\n",
    "                'Production Index Italy: Electrical equipment',\n",
    "                'Production Index Japan: Electrical equipment',\n",
    "                'Production Index France: Electrical equipment','Production Index Germany: Electrical equipment']\n",
    "\n",
    "market_db.columns = new_col_names\n",
    "market_db.drop(0, inplace=True)\n",
    "market_db.drop(1, inplace=True)\n",
    "market_db.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the group aligned the market dataset with the Sales dataset to make sure the indicators matched with the date range (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db['DATE'] = pd.to_datetime(market_db['DATE'].str.strip().str.replace('m', '-'), format='%Y-%m')\n",
    "market_db['DATE'] = market_db['DATE'].dt.to_period('M')\n",
    "market_db = market_db[market_db['DATE'] >= '2018-10']\n",
    "market_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sales were in EUR but some macroeconomic indicators like commodity prices are quoted in USD, the group decided to convert them into the same currency by dividing it by the exchange rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_columns = [\n",
    "    'World: Price of Base Metals', \n",
    "    'World: Price of Energy', \n",
    "    'World: Price of Metals & Minerals',\n",
    "    'World: Price of Natural gas index', \n",
    "    'World: Price of Crude oil, average', \n",
    "    'World: Price of Copper',\n",
    "    'Producer Prices United States: Electrical equipment', \n",
    "    'Producer Prices United Kingdom: Electrical equipment',\n",
    "    'Producer Prices Italy: Electrical equipment', \n",
    "    'Producer Prices France: Electrical equipment',\n",
    "    'Producer Prices Germany: Electrical equipment', \n",
    "    'Producer Prices China: Electrical equipment'\n",
    "]\n",
    "\n",
    "for col in price_columns:\n",
    "    market_db[col] = market_db[col] / market_db['United States: EUR in LCU']\n",
    "\n",
    "market_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = market_db.isnull().sum()\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the 2 columns with 18 missing rows each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_list = nulls[nulls > 1].index.tolist()\n",
    "nulls_list\n",
    "for col in nulls_list:\n",
    "    missing_dates = market_db[market_db[col].isnull()]['DATE'].dt.strftime('%Y-%m').unique()\n",
    "    print(f\"Missing dates for {col}: {missing_dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing rows seems to be from \"connected dates\". The missing values started on October 2020 and end on April 2022, this is - about - half of the data and as such these features are best removed than treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group also decided to remove the column Shipments China because it was exactly the same as the Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_db = market_db.drop(columns=nulls_list + ['Shipments Index Machinery & Electricals China'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before handling the rest of the columns with missing values and the outliers, the group decided to split the dataset in train (80%) and validation (20%) preserving the chronological order of the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(market_db) * 0.8)\n",
    "market_train = market_db[:train_size]\n",
    "market_validation = market_db[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_val = sales_db[sales_db['DATE'] >= '2021-08']\n",
    "sales_train = sales_db[sales_db['DATE'] < '2021-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_market_val = market_validation.isnull().sum()\n",
    "nulls_market_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets treat the columns with one missing row by replacing it with the average for that index in that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_list = nulls_market_val[nulls_market_val > 0].index.tolist()\n",
    "\n",
    "def filler(df, cols):\n",
    "    for col in cols:\n",
    "        if df[col].isna().sum() == 1:\n",
    "            year = df[df[col].isnull()]['DATE'].dt.year.values[0]\n",
    "            avg = df[df['DATE'].dt.year == year][col].mean()\n",
    "            df[col].fillna(avg, inplace=True)\n",
    "    return df\n",
    "\n",
    "market_validation = filler(market_validation, nulls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [market_train, market_validation]:\n",
    "    obj_cols = df.select_dtypes(include='object').columns\n",
    "    df[obj_cols] = df[obj_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_validation.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train = sales_train.copy()\n",
    "trans_val = sales_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = sales_train.merge(market_train, on = \"DATE\", how = \"left\")\n",
    "val_df = sales_val.merge(market_validation, on=\"DATE\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ids_to_check = ['#1', '#3', '#4', '#5', '#8', '#9', '#13', '#14', '#16' '#20', '#36']\n",
    "\n",
    "filtered_train = train_df.copy()\n",
    "filtered_trans = trans_train.copy()\n",
    "\n",
    "for product_id in product_ids_to_check:\n",
    "    \n",
    "    product_data = filtered_train[filtered_train['Mapped_GCK'] == product_id]\n",
    "    product_data1 = filtered_trans[filtered_trans['Mapped_GCK'] == product_id]\n",
    "    \n",
    "    Q1 = product_data['Sales_EUR'].quantile(0.25)\n",
    "    Q3 = product_data['Sales_EUR'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    Q11 = product_data1['Sales_EUR'].quantile(0.25)\n",
    "    Q31 = product_data1['Sales_EUR'].quantile(0.75)\n",
    "    IQR1 = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.6 * IQR\n",
    "    upper_bound = Q3 + 1.6 * IQR\n",
    "    \n",
    "    lower_bound = Q11 - 1.6 * IQR1\n",
    "    upper_bound = Q31 + 1.6 * IQR1\n",
    "\n",
    "    for idx, row in product_data.iterrows():\n",
    "        if row['Sales_EUR'] < lower_bound:\n",
    "            filtered_train.at[idx, 'Sales_EUR'] = lower_bound  \n",
    "            filtered_trans.at[idx, 'Sales_EUR'] = lower_bound\n",
    "        elif row['Sales_EUR'] > upper_bound:\n",
    "            filtered_train.at[idx, 'Sales_EUR'] = upper_bound  \n",
    "            filtered_trans.at[idx, 'Sales_EUR'] = lower_bound\n",
    "\n",
    "train_df = filtered_train\n",
    "trans_train = filtered_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_gck_list = val_df['Mapped_GCK'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_product_dfs = {}  \n",
    "val_product_dfs = {}\n",
    "train_for_export = {}\n",
    "val_for_export = {}\n",
    "\n",
    "for i in mapped_gck_list:\n",
    "    train_product_dfs[i] = train_df[train_df[\"Mapped_GCK\"] == i]\n",
    "    val_product_dfs[i] = val_df[val_df[\"Mapped_GCK\"] == i]\n",
    "    train_for_export[i] = train_df[train_df[\"Mapped_GCK\"] == i]\n",
    "    val_for_export[i] = val_df[val_df[\"Mapped_GCK\"] == i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the non-stationary series stationary, the group applied differencing, which removes trends and leaves only seasonal variations. Which helps when using models that assume stationarity. This process required the data to be merged and separated again, as it leads to the loss of one row and applying it in hte validation set would mean the loss of a step in the middle of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = pd.concat([train_df, val_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dfs = {}\n",
    "\n",
    "for i in mapped_gck_list:\n",
    "    train_val_dfs[i] = train_val[train_val[\"Mapped_GCK\"] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series, alpha=0.05):\n",
    "    result = ts.adfuller(series.dropna(), autolag='AIC')\n",
    "    p_value = result[1]\n",
    "    return p_value < alpha, p_value\n",
    "\n",
    "transformations = {}\n",
    "\n",
    "for product in train_val_dfs.keys():\n",
    "    print(f\"\\nProcessing Product: {product}\")\n",
    "    \n",
    "    df_train = train_val_dfs[product].copy()\n",
    "    \n",
    "    transformations[product] = {}\n",
    "    \n",
    "    exog_cols = [col for col in df_train.columns if col not in ['DATE', 'Mapped_GCK', 'Sales_EUR']]\n",
    "    \n",
    "    for col in exog_cols:\n",
    "        is_stat, p_val = adf_test(df_train[col])\n",
    "        if not is_stat:\n",
    "            df_train[col] = df_train[col].diff()\n",
    "            transformations[product][col] = 'first_difference'\n",
    "            print(f\"Column '{col}' for {product} non-stationary (p={p_val:.4f}). Applied differencing.\")\n",
    "        else:\n",
    "            transformations[product][col] = 'none'\n",
    "            print(f\"Column '{col}' for {product} is stationary (p={p_val:.4f}).\")\n",
    "    \n",
    "    df_train.dropna(inplace=True)\n",
    "    train_val_dfs[product] = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prod in train_val_dfs.keys():\n",
    "    train_product_dfs[prod] = train_val_dfs[prod][train_val_dfs[prod]['DATE'] < '2021-08']\n",
    "    val_product_dfs[prod] = train_val_dfs[prod][train_val_dfs[prod]['DATE'] >= '2021-08']\n",
    "\n",
    "for prod in train_val_dfs.keys():\n",
    "    train_for_export[prod] = train_val_dfs[prod][train_val_dfs[prod]['DATE'] < '2022-01']\n",
    "    val_for_export[prod] = train_val_dfs[prod][train_val_dfs[prod]['DATE'] >= '2022-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature selection phase, the group decided to use the correlation matrix to identify the variables with a correlation higher than 0.90, the VIF, the Granger causality test to identify features that influence the target and feature importance with Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in train_product_dfs:\n",
    "    train_product_dfs[key]['DATE'] = pd.to_datetime(train_product_dfs[key]['DATE'].astype(str))\n",
    "    train_product_dfs[key].set_index('DATE', inplace=True)\n",
    "\n",
    "    val_product_dfs[key]['DATE'] = pd.to_datetime(val_product_dfs[key]['DATE'].astype(str))\n",
    "    val_product_dfs[key].set_index('DATE', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "The group checked the subset of features that were highly correlated with each other and decided which ones to keep manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Processing Product: {gck}\")\n",
    "    \n",
    "    train_gck = train_product_dfs[gck].drop(columns=['Mapped_GCK'])\n",
    "    X_train = train_gck.drop(columns=['Sales_EUR'])\n",
    "\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    high_corr_pairs = []\n",
    "    for column in upper.columns:\n",
    "        high_corr = upper[column][upper[column] > 0.9]\n",
    "        for idx in high_corr.index:\n",
    "            high_corr_pairs.append((column, idx, high_corr[idx]))\n",
    "\n",
    "    if high_corr_pairs:\n",
    "        print(\"Highly correlated feature pairs (correlation > 0.9):\")\n",
    "        for feature1, feature2, corr_value in high_corr_pairs:\n",
    "            print(f\"{feature1} ↔ {feature2} (Correlation: {corr_value:.2f})\")\n",
    "    else:\n",
    "        print(\"No highly correlated feature pairs found.\")\n",
    "\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features to drop:\n",
    "- Production Index Machinery & Electricals Europe\n",
    "- Shipments Index Machinery & Electricals Europe\n",
    "- Shipments Index Machinery & Electricals Switzerland\n",
    "- Shipments Index Machinery & Electricals Japan\n",
    "- Shipments Index Machinery & Electricals Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- World: Price of Crude oil, average\n",
    "- World: Price of Base Metals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- United States: EUR in LCU\n",
    "- Producer Prices Italy: Electrical equipment\n",
    "- Producer Prices France: Electrical equipment\n",
    "- Producer Prices United States: Electrical equipment\n",
    "- The group decided to retain only \"Producer Prices Germany: Electrical equipment\" because the other producer price indicators were highly correlated with each other and using a local producer price index ensures that your model reflects the environment in which the business unit operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Production Index United States: Machinery and equipment n.e.c.\n",
    "- Prodcution Index World: Machinery and equipment n.e.c.\n",
    "- Production Index Switzerland: Machinery and equipment n.e.c.\n",
    "- Production Index United Kingdom: Machinery and equipment n.e.c.\n",
    "- Production Index Italy: Machinery and equipment n.e.c.\n",
    "- Production Index  Japan: Machinery and equipment n.e.c.\n",
    "- Production Index France: Machinery and equipment n.e.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Production Index World: Electrical equipment\n",
    "- Production Index Switzerland: Electrical equipment\n",
    "- Production Index Italy: Electrical equipment\n",
    "- Production Index Japan: Electrical equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [\n",
    "    \"Production Index Machinery & Electricals Europe\",\n",
    "    \"Shipments Index Machinery & Electricals Europe\",\n",
    "    \"Shipments Index Machinery & Electricals Switzerland\",\n",
    "    \"Shipments Index Machinery & Electricals Japan\",\n",
    "    \"Shipments Index Machinery & Electricals Germany\",\n",
    "    \"World: Price of Crude oil, average\",\n",
    "    \"World: Price of Base Metals\",\n",
    "    \"United States: EUR in LCU\",\n",
    "    \"Producer Prices Italy: Electrical equipment\",\n",
    "    \"Producer Prices France: Electrical equipment\",\n",
    "    \"Producer Prices United States: Electrical equipment\",\n",
    "    \"Production Index United States: Machinery and equipment n.e.c.\",\n",
    "    \"Prodcution Index World: Machinery and equipment n.e.c.\",\n",
    "    \"Production Index Switzerland: Machinery and equipment n.e.c.\",\n",
    "    \"Production Index United Kingdom: Machinery and equipment n.e.c.\",\n",
    "    \"Production Index Italy: Machinery and equipment n.e.c.\",\n",
    "    \"Production Index  Japan: Machinery and equipment n.e.c.\",\n",
    "    \"Production Index France: Machinery and equipment n.e.c.\",\n",
    "    \"Production Index World: Electrical equipment\",\n",
    "    \"Production Index Switzerland: Electrical equipment\",\n",
    "    \"Production Index Italy: Electrical equipment\",\n",
    "    \"Production Index Japan: Electrical equipment\"\n",
    "]\n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    train_product_dfs[gck] = train_product_dfs[gck].drop(columns=features_to_drop)\n",
    "    val_product_dfs[gck] = val_product_dfs[gck].drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF\n",
    "VIF feature selection identifies and removes highly collinear variables by calculating how much a predictor (a variable) is inflated by correlations with other predictors. Features with high VIF values (in this case >10) are removed to reduce multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_vif(data, threshold=10):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    print(\"VIF values:\\n\", vif_data)\n",
    "    return vif_data[vif_data[\"VIF\"] < threshold][\"feature\"].tolist()\n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Selecting features for Product: {gck}\")\n",
    "    train_gck = train_product_dfs[gck].drop(columns=['Mapped_GCK'])\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    X_train = train_gck.drop(columns=['Sales_EUR'])\n",
    "    selected_features = high_vif(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger Test\n",
    "Next, the group used the granger causality test to determine wheter one time series can predict the future values of another time series. \"It measures the extent to which the past values of one variable provide valuable information for forecasting the other variable’s future behavior\" (Padav, 2024)<sup>1</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def granger_test(data, target, max_lag=5, significance=0.05):\n",
    "    selected_features = []\n",
    "    for col in data.columns:\n",
    "        test_result = grangercausalitytests(data[[target, col]], max_lag, verbose=False)\n",
    "        p_values = [test_result[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]\n",
    "        if min(p_values) < significance:\n",
    "            selected_features.append(col)\n",
    "    return selected_features\n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Selecting features for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_product_dfs[gck].drop(columns=['Mapped_GCK'])\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    X_train = train_gck.drop(columns=['Sales_EUR'])\n",
    "    selected_features = granger_test(train_gck, 'Sales_EUR')\n",
    "    print(\"Features selected by Granger Causality Test:\", selected_features, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide the final indicators for each product, the group used feature importance using Random Forest which identifies the most influential features. (Note: It was used a threshold for each product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(X, y):\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    return rf.feature_importances_\n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Selecting features for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_product_dfs[gck].drop(columns=['Mapped_GCK'])\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    X_train = train_gck.drop(columns=['Sales_EUR'])\n",
    "\n",
    "    rf_importance = feature_importance(X_train, y_train)\n",
    "    rf_features = X_train.columns[np.argsort(rf_importance)[::-1]].tolist()\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": rf_features,\n",
    "        \"Random_Forest_Importance\": rf_importance\n",
    "    })\n",
    "\n",
    "    importance_df = importance_df.sort_values(by = \"Random_Forest_Importance\", ascending= False)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.barh(importance_df[\"Feature\"], importance_df[\"Random_Forest_Importance\"], color=\"#009999\", label=\"Random Forest\")\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.title(\"Feature Importance Random Forest\")\n",
    "    plt.legend()\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exogenous Variables for Each GCK\n",
    "The group used an excel file (available in the [github](https://github.com/mikefferreira/BusinessCases)) to the decide the features to keep and drop.\n",
    "\n",
    "- **#1**:\n",
    "  - Production Index Machinery & Electricals China  \n",
    "  - Production Index Machinery & Electricals Switzerland  \n",
    "  - World: Price of Energy  \n",
    "  - Production Index United States: Electrical equipment  \n",
    "  - Production Index France: Electrical equipment  \n",
    "\n",
    "- **#11**:  \n",
    "  - World: Price of Metals & Minerals  \n",
    "\n",
    "- **#12**:  \n",
    "  - Shipments Index Machinery & Electricals France  \n",
    "  - Production Index Machinery & Electricals Italy  \n",
    "  - Production Index United States: Electrical equipment  \n",
    "\n",
    "- **#13**:  \n",
    "  - None  \n",
    "\n",
    "- **#14**:  \n",
    "  - Production Index Machinery & Electricals Switzerland  \n",
    "  - World: Price of Energy  \n",
    "\n",
    "- **#20**:  \n",
    "  - Production Index Machinery & Electricals China  \n",
    "  - Production Index Machinery & Electricals Germany  \n",
    "  - Production Index Machinery & Electricals Japan  \n",
    "  - World: Price of Copper  \n",
    "  - World: Price of Metals & Minerals  \n",
    "  - World: Price of Natural gas index  \n",
    "  - Production Index United Kingdom: Electrical equipment  \n",
    "\n",
    "- **#3**:  \n",
    "  - Production Index Machinery & Electricals United States  \n",
    "  - World: Price of Copper  \n",
    "\n",
    "- **#36**:  \n",
    "  - Production Index Machinery & Electricals United States  \n",
    "\n",
    "- **#4**:  \n",
    "  - Production Index Machinery & Electricals Germany  \n",
    "  - Production Index Machinery & Electricals Japan  \n",
    "  - World: Price of Natural gas index  \n",
    "  - Production Index Germany: Machinery and equipment n.e.c.  \n",
    "\n",
    "- **#5**:  \n",
    "  - Production Index Machinery & Electricals Germany  \n",
    "  - Production Index Machinery & Electricals Japan  \n",
    "  - Producer Prices Germany: Electrical equipment  \n",
    "  - Production Index United States: Electrical equipment  \n",
    "\n",
    "- **#6**:  \n",
    "  - Production Index Machinery & Electricals China  \n",
    "  - World: Price of Copper  \n",
    "  - World: Price of Energy  \n",
    "\n",
    "- **#8**:  \n",
    "  - Production Index Machinery & Electricals China  \n",
    "  - Shipments Index Machinery & Electricals Italy  \n",
    "  - Production Index Machinery & Electricals Switzerland  \n",
    "  - World: Price of Copper  \n",
    "  - Production Index Germany: Machinery and equipment n.e.c.  \n",
    "  - Production Index United States: Electrical equipment  \n",
    "\n",
    "- **#9**:  \n",
    "  - World: Price of Copper  \n",
    "\n",
    "- **#16**:  \n",
    "  - None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA (without exogenous variables)\n",
    "\n",
    "\"​An Autoregressive Integrated Moving Average (ARIMA) model is a statistical analysis tool that utilizes time series data to either better understand the dataset or to predict future trends.\"(Adam Hayes, 2024)<sup>2</sup> \n",
    "\n",
    "This model is quite simplistic and it served as a benchmark before adding macroeconomic indicators and moving to more complex - and hopefully better performing - models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Training ARIMA for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_product_dfs[gck]\n",
    "    val_gck = val_product_dfs[gck]\n",
    "    \n",
    "    train_gck = train_gck.drop(columns=['Mapped_GCK'])\n",
    "    val_gck = val_gck.drop(columns=['Mapped_GCK'])\n",
    "    \n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    y_val = val_gck['Sales_EUR']\n",
    "\n",
    "    model = ARIMA(y_train, order=(1, 1, 1))\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + len(y_val) - 1)\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    results[gck] = {'rmse': rmse, 'y_val': y_val, 'y_pred': y_pred}\n",
    "    \n",
    "    print(f\"Product: {gck}, RMSE: {rmse}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_val.index, y_val, label='Actual Sales')\n",
    "    plt.plot(y_val.index, y_pred, label='Predicted Sales', linestyle='dashed', color='#2D373C')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales (EUR)')\n",
    "    plt.title(f'ARIMA Model - Actual vs Predicted Sales for {gck}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for gck, result in results.items():\n",
    "    print(f\"Product: {gck}, RMSE: {result['rmse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model produced poor results; however, as previously stated, it served as only has a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMAX (using all exogenous variables)\n",
    "\n",
    "\"The Seasonal Autoregressive Integrated Moving Average with Exogenous Regressors (SARIMAX) model is a powerful time series forecasting technique that extends the traditional ARIMA model to account for seasonality and external factors. It's a versatile model that can accommodate both autoregressive (AR) and moving average (MA) components, integrate differencing to make the data stationary, and incorporate external variables or regressors. SARIMAX is particularly valuable when dealing with time-dependent data that exhibits recurring patterns over specific time intervals.\"(GeekforGeeks, 2023)<sup>3</sup>\n",
    "\n",
    "Before trying a SARIMAX model with the feature list that was created, the group decided to create yet another baseline prediction where all possible features were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Training SARIMAX for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_product_dfs[gck]\n",
    "    val_gck = val_product_dfs[gck]\n",
    "\n",
    "    train_gck = train_gck.drop(columns=['Mapped_GCK'])\n",
    "    val_gck = val_gck.drop(columns=['Mapped_GCK'])\n",
    "\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    X_train = train_gck.drop(columns=['Sales_EUR'])\n",
    "\n",
    "    y_val = val_gck['Sales_EUR']\n",
    "    X_val = val_gck.drop(columns=['Sales_EUR'])\n",
    "\n",
    "    model = SARIMAX(y_train, exog=X_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "    model_fit = model.fit(disp=False)\n",
    "\n",
    "    y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + len(y_val) - 1, exog=X_val)\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "    results[gck] = {'rmse': rmse, 'y_val': y_val, 'y_pred': y_pred}\n",
    "\n",
    "    print(f\"Product: {gck}, RMSE: {rmse}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_val.index, y_val, label='Actual Sales')\n",
    "    plt.plot(y_val.index, y_pred, label='Predicted Sales', linestyle='dashed', color='#2D373C')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales (EUR)')\n",
    "    plt.title(f'SARIMAX Model - Actual vs Predicted Sales for {gck}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for gck, result in results.items():\n",
    "    print(f'Product: {gck}, RMSE: {result[\"rmse\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After incorporating all exogenous variables, the group observed that the overall results were not optimal. However, for certain products, the RMSE values were within a reasonable range, indicating some potential benefits of including the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group then proceeded with the SARIMAX model using the previously selected features. However, since the feature selection process indicated that no additional variables were needed for products #16 and #13, these were modeled without exogenous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "gck_exog_vars = {\n",
    "    '#1': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Energy', 'Production Index United States: Electrical equipment', 'Production Index France: Electrical equipment'],\n",
    "    '#11': ['World: Price of Metals & Minerals'],\n",
    "    '#12': ['Shipments Index Machinery & Electricals France', 'Production Index Machinery & Electricals Italy', 'Production Index United States: Electrical equipment'],\n",
    "    '#13': None,\n",
    "    '#14': ['Production Index Machinery & Electricals Switzerland', 'World: Price of Energy'],\n",
    "    '#20': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Copper', 'World: Price of Metals & Minerals', 'World: Price of Natural gas index', 'Production Index United Kingdom: Electrical equipment'],\n",
    "    '#3': ['Production Index Machinery & Electricals United States', 'World: Price of Copper'],\n",
    "    '#36': ['Production Index Machinery & Electricals United States'],\n",
    "    '#4': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Natural gas index', 'Production Index Germany: Machinery and equipment n.e.c.'],\n",
    "    '#5': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'Producer Prices Germany: Electrical equipment', 'Production Index United States: Electrical equipment'],\n",
    "    '#6': ['Production Index Machinery & Electricals China', 'World: Price of Copper', 'World: Price of Energy'],\n",
    "    '#8': ['Production Index Machinery & Electricals China', 'Shipments Index Machinery & Electricals Italy', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Copper', 'Production Index Germany: Machinery and equipment n.e.c.', 'Production Index United States: Electrical equipment'],\n",
    "    '#9': ['World: Price of Copper'],\n",
    "    '#16' : None,\n",
    "}\n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Training ARIMAX for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_product_dfs[gck]\n",
    "    val_gck = val_product_dfs[gck]\n",
    "\n",
    "    train_gck = train_gck.drop(columns=['Mapped_GCK'])\n",
    "    val_gck = val_gck.drop(columns=['Mapped_GCK'])\n",
    "\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    y_val = val_gck['Sales_EUR']\n",
    "\n",
    "    if gck_exog_vars[gck] is not None:\n",
    "        X_train = train_gck[gck_exog_vars[gck]]\n",
    "        X_val = val_gck[gck_exog_vars[gck]]\n",
    "\n",
    "        model = SARIMAX(y_train, exog=X_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "\n",
    "        y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + len(y_val) - 1, exog=X_val)\n",
    "    else:\n",
    "        model = ARIMA(y_train, order=(1, 1, 1))\n",
    "        model_fit = model.fit()\n",
    "        y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + len(y_val) - 1)\n",
    "\n",
    "    y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + len(y_val) - 1, exog=X_val)\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    results[gck] = {'rmse': rmse, 'y_val': y_val, 'y_pred': y_pred}\n",
    "\n",
    "    print(f\"Product: {gck}, RMSE: {rmse}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_val.index, y_val, label='Actual Sales')\n",
    "    plt.plot(y_val.index, y_pred, label='Predicted Sales', linestyle='dashed', color='#2D373C')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales (EUR)')\n",
    "    plt.title(f'ARIMAX Model - Actual vs Predicted Sales for {gck}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for gck, result in results.items():\n",
    "    print(f'Product: {gck}, RMSE: {result[\"rmse\"]}')\n",
    "\n",
    "sarimax_rmse = {gck: result['rmse'] for gck, result in results.items() if 'rmse' in result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the group implemented SARIMAX with selected exogenous features for each product and an ARIMAX for the products #13 and #16:\n",
    "\n",
    "Best Performing Models (Lowest RMSE):\n",
    "- Product #20 (RMSE: 3671)\n",
    "- Product #9 (RMSE: 8973)\n",
    "- Product #13 (RMSE: 10044)\n",
    "- Product #14 (RMSE: 14513)\n",
    "- Product: #36 (RMSE: 16894)\n",
    "- Product #16 (RMSE: 88102)\n",
    "\n",
    "These products exhibited strong predictive performance, suggesting that the selected exogenous variables effectively contributed to capturing demand patterns.\n",
    "\n",
    "Moderate Performance:\n",
    "- Products such as #11 (RMSE: 644670), #12 (RMSE: 317041), #8 (RMSE: 622064), #6 (RMSE: 380950), and #4 (RMSE: 419306) had RMSE values in the mid-range.\n",
    "\n",
    "Underperforming Models (Highest RMSE):\n",
    "- Product #1 (RMSE: 6294103)\n",
    "- Product #5 (RMSE: 4911513)\n",
    "- Product #3 (RMSE: 2225187)\n",
    "\n",
    "The high RMSE values for these products suggest that the selected exogenous variables might not be sufficient to explain the variation in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "\"XGBoost is short for Extreme Gradient Boosting and is an efficient implementation of the stochastic gradient boosting machine learning algorithm. The stochastic gradient boosting algorithm, also called gradient boosting machines or tree boosting, is a powerful machine learning technique that performs well or even best on a wide range of challenging machine learning problems.\" (Jason Brownlee, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in train_product_dfs.keys():\n",
    "    train_product_dfs[product] = train_product_dfs[product].reset_index()\n",
    "    val_product_dfs[product] = val_product_dfs[product].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.models import XGBModel\n",
    "from darts.metrics import rmse\n",
    "from darts.models.forecasting.xgboost import XGBModel\n",
    "\n",
    "forecasts = {}\n",
    "rmse_scores = {}\n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Training XGBoost for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_product_dfs[gck].copy()\n",
    "    val_gck = val_product_dfs[gck].copy()\n",
    "\n",
    "    train_gck = train_gck.drop(columns=['Mapped_GCK'])\n",
    "    val_gck = val_gck.drop(columns=['Mapped_GCK'])\n",
    "\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    y_val = val_gck['Sales_EUR']\n",
    "\n",
    "    target_train = TimeSeries.from_dataframe(train_gck, time_col='DATE', value_cols=['Sales_EUR'])\n",
    "    target_val = TimeSeries.from_dataframe(val_gck, time_col='DATE', value_cols=['Sales_EUR'])\n",
    "\n",
    "    if gck in selected_features:\n",
    "        X_train = train_gck[selected_features[gck]]\n",
    "        X_val = val_gck[selected_features[gck]]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        exog_train = TimeSeries.from_dataframe(train_gck, time_col='DATE', value_cols=selected_features[gck])\n",
    "        exog_val = TimeSeries.from_dataframe(val_gck, time_col='DATE', value_cols=selected_features[gck])\n",
    "    else:\n",
    "        exog_train, exog_val = None, None  \n",
    "\n",
    "\n",
    "    model = XGBModel(lags=9, lags_exog=9 if exog_train else None)\n",
    "    model.fit(target_train, past_covariates=exog_train)\n",
    "\n",
    "    y_pred = model.predict(n=9, past_covariates=exog_val)\n",
    "\n",
    "    forecasts[gck] = y_pred\n",
    "\n",
    "    rmse_value = rmse(target_val, y_pred)\n",
    "    rmse_scores[gck] = rmse_value\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    target_val.plot(label=\"Actual Sales\", linestyle=\"-\", color = '#009999')\n",
    "    y_pred.plot(label=\"Predicted Sales\", linestyle=\"--\", color= '#2D373C')\n",
    "    plt.title(f\"Product {gck}: Actual vs Predicted Sales (RMSE: {rmse_value:.2f})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Sales_EUR\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "for gck, rmse_value in rmse_scores.items():\n",
    "    print(f'Product: {gck}, RMSE: {rmse_value}')\n",
    "\n",
    "xgb_rmse = {gck: rmse_value for gck, rmse_value in rmse_scores.items() if rmse_value is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the group implemented XGBoost with selected exogenous features for each product:\n",
    "\n",
    "Best Performing Models (Lowest RMSE):\n",
    "- Product #9 (RMSE: 7156)\n",
    "- Product #20 (RMSE: 3975)\n",
    "- Product #13 (RMSE: 15815)\n",
    "- Product #14 (RMSE: 13729)\n",
    "- Product #36 (RMSE: 16268)\n",
    "- Product #16 (RMSE: 72988)\n",
    "\n",
    "Moderate Performance:\n",
    "- Products such as #12 (RMSE: 159161), #6 (RMSE: 319482), #4 (RMSE: 192749), #8 (RMSE: 705728), and #11 (RMSE: 1449084) had RMSE values in the mid-range.\n",
    "\n",
    "Underperforming Models (Highest RMSE):\n",
    "- Product #1 (RMSE: 2881129)\n",
    "- Product #3 (RMSE: 2828943)\n",
    "- Product #5 (RMSE: 5272665)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "\"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\" (GitHub, n.d.)<sup>4</sup>\n",
    "\n",
    "This model is open-source software released by Meta (formerly known as Facebook) and is widely known for its efficiency and proficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_results = {}\n",
    "\n",
    "for product in mapped_gck_list:\n",
    "    print(f\"Processing Product: {product}\")\n",
    "    \n",
    "    df_train = train_product_dfs[product].reset_index()\n",
    "    df_val   = val_product_dfs[product].reset_index()\n",
    "    \n",
    "    df_prophet = df_train[['DATE', 'Sales_EUR']].rename(columns={'DATE': 'ds', 'Sales_EUR': 'y'})\n",
    "    \n",
    "    model = Prophet(seasonality_mode='multiplicative', yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "    model.fit(df_prophet)\n",
    "    \n",
    "    forecast_periods = len(df_val)\n",
    "    future = model.make_future_dataframe(periods=forecast_periods, freq='MS')\n",
    "    \n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    y_pred = forecast['yhat'][-forecast_periods:]\n",
    "\n",
    "    mse = mean_squared_error(df_val['Sales_EUR'], y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    prophet_results[product] = {'rmse': rmse, 'y_val': df_val['Sales_EUR'], 'y_pred': y_pred}\n",
    "\n",
    "    print(f\"Product: {product}, RMSE: {rmse}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_val['DATE'], df_val['Sales_EUR'], label='Actual Sales', color = '#009999')\n",
    "    plt.plot(df_val['DATE'], y_pred, label='Predicted Sales', linestyle='dashed', color='#2D373C')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales (EUR)')\n",
    "    plt.title(f'Prophet Model - Actual vs Predicted Sales for {product}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for product, result in prophet_results.items():\n",
    "    print(f\"Product: {product}, RMSE: {result['rmse']}\")\n",
    "\n",
    "prophet_rmse = {gck: rmse_value for gck, rmse_value in prophet_results.items() if rmse_value is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the group implemented Prophet\n",
    "\n",
    "Best Performing Models (Lowest RMSE):\n",
    "- Product #20 (RMSE: 4371)\n",
    "- Product #9 (RMSE: 12507)\n",
    "- Product #13 (RMSE: 15307)\n",
    "- Product #14 (RMSE: 14480)\n",
    "- Product #36 (RMSE: 16034)\n",
    "\n",
    "\n",
    "Moderate Performance:\n",
    "- Products such as #12 (RMSE: 231233), #6 (RMSE: 279889), #16 (RMSE: 356636), #4 (RMSE: 179709), #8 (RMSE: 544612), and #11 (RMSE: 704133) had RMSE values in the mid-range.\n",
    "\n",
    "Underperforming Models (Highest RMSE):\n",
    "- Product #1 (RMSE: 4243981)\n",
    "- Product #5 (RMSE: 4247702)\n",
    "- Product #3 (RMSE: 2477892)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMEGPT\n",
    "\n",
    "\"TimeGPT is a production-ready generative pretrained transformer for time series. It’s capable of accurately predicting various domains such as retail, electricity, finance, and IoT with just a few lines of code.\" (NIXTLA, 2024)<sup>5</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client = NixtlaClient(\n",
    "    api_key = 'nixak-IJDy3xU0F51O6p8oD2ZxTm1f0qOuwPofNu8bXP1NpIpGj8O5uVN8WE3YecfIJh0DLMi2AMof6672XL4t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train['DATE'] = pd.to_datetime(trans_train['DATE'].dt.to_timestamp())\n",
    "trans_val['DATE'] = pd.to_datetime(trans_val['DATE'].dt.to_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegpt_fcst_df = nixtla_client.forecast(trans_train, time_col='DATE', target_col='Sales_EUR', id_col='Mapped_GCK', h = 9, freq='MS', model='timegpt-1-long-horizon')\n",
    "timegpt_fcst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_gcks = timegpt_fcst_df['Mapped_GCK'].unique()\n",
    "\n",
    "for gck in unique_gcks:\n",
    "    fig, ax = plt.subplots(figsize=(18, 4))\n",
    "    sns.set_style('whitegrid')\n",
    "    \n",
    "    actual_data = trans_val[trans_val['Mapped_GCK'] == gck]\n",
    "    forecasted_data = timegpt_fcst_df[timegpt_fcst_df['Mapped_GCK'] == gck]\n",
    "    \n",
    "    sns.lineplot(x='DATE', y='Sales_EUR', data=actual_data, color='#009999', label='Actual Sales')\n",
    "    sns.lineplot(x='DATE', y='TimeGPT', data=forecasted_data, color='#2D373C', label='Forecasted Sales', linestyle='dashed')\n",
    "    \n",
    "    sns.despine(bottom=True, left=True)\n",
    "    plt.title(f'Actual vs Forecasted Sales for GCK {gck}', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Sales (EUR)', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegpt_rmse = {}\n",
    "for gck in unique_gcks:\n",
    "    actual_sales = trans_val[trans_val['Mapped_GCK'] == gck]['Sales_EUR']\n",
    "    forecasted_sales = timegpt_fcst_df[timegpt_fcst_df['Mapped_GCK'] == gck]['TimeGPT']\n",
    "    timegpt_rmse[gck] = root_mean_squared_error(actual_sales, forecasted_sales)\n",
    "\n",
    "timegpt_rmse = {gck: rmse_value for gck, rmse_value in timegpt_rmse.items() if rmse_value is not None}\n",
    "for i in timegpt_rmse:\n",
    "    print(i, timegpt_rmse[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the group implemented TimeGPT:\n",
    "\n",
    "Best Performing Models (Lowest RMSE):\n",
    "- Product #20 (RMSE: 1939) – Best overall performance\n",
    "- Product #9 (RMSE: 8737)\n",
    "- Product #13 (RMSE: 16146)\n",
    "- Product #36 (RMSE: 16070)\n",
    "\n",
    "Moderate Performance:\n",
    "- Products such as #12 (RMSE: 146846), #16 (RMSE: 91280), #6 (RMSE: 267882), #4 (RMSE: 69556), #8 (RMSE: 544870), and #11 (RMSE: 696550) had RMSE values in the mid-range.\n",
    "\n",
    "Underperforming Models (Highest RMSE):\n",
    "- Product #1 (RMSE: 3725921)\n",
    "- Product #5 (RMSE: 4162117)\n",
    "- Product #3 (RMSE: 2064101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Forecast\n",
    "\n",
    "\"NeuralForecast offers a large collection of neural forecasting models focusing on their performance, usability, and robustness.\" (Github, n.d.)<sup>5</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gck_features = {\n",
    "    '#1': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Energy', 'Production Index United States: Electrical equipment', 'Production Index France: Electrical equipment'],\n",
    "    '#11': ['World: Price of Metals & Minerals'],\n",
    "    '#12': ['Shipments Index Machinery & Electricals France', 'Production Index Machinery & Electricals Italy', 'Production Index United States: Electrical equipment'],\n",
    "    '#13': None,\n",
    "    '#14': ['Production Index Machinery & Electricals Switzerland', 'World: Price of Energy'],\n",
    "    '#20': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Copper', 'World: Price of Metals & Minerals', 'World: Price of Natural gas index', 'Production Index United Kingdom: Electrical equipment'],\n",
    "    '#3': ['Production Index Machinery & Electricals United States', 'World: Price of Copper'],\n",
    "    '#36': ['Production Index Machinery & Electricals United States'],\n",
    "    '#4': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Natural gas index', 'Production Index Germany: Machinery and equipment n.e.c.'],\n",
    "    '#5': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'Producer Prices Germany: Electrical equipment', 'Production Index United States: Electrical equipment'],\n",
    "    '#6': ['Production Index Machinery & Electricals China', 'World: Price of Copper', 'World: Price of Energy'],\n",
    "    '#8': ['Production Index Machinery & Electricals China', 'Shipments Index Machinery & Electricals Italy', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Copper', 'Production Index Germany: Machinery and equipment n.e.c.', 'Production Index United States: Electrical equipment'],\n",
    "    '#9': ['World: Price of Copper'],\n",
    "    '#16' : None\n",
    "    }\n",
    "\n",
    "gck_predictions = {}\n",
    "\n",
    "for gck, features in gck_features.items():\n",
    "    print(f\"Processing GCK: {gck}\")\n",
    "    \n",
    "    gck_train = train_product_dfs[gck].reset_index()\n",
    "    gck_train = gck_train.rename(columns={'DATE': 'ds', 'Sales_EUR': 'y', 'Mapped_GCK': 'unique_id'})\n",
    "    gck_train = gck_train[['ds', 'y', 'unique_id'] + (features if features else [])]\n",
    "    \n",
    "    models = [\n",
    "        NBEATS(input_size=2 * 10, h=10, max_steps=100, enable_progress_bar=False),\n",
    "        NHITS(input_size=2 * 10, h=10, max_steps=100, enable_progress_bar=False)\n",
    "    ]\n",
    "    nf = NeuralForecast(models=models, freq='MS')\n",
    "    \n",
    "    nf.fit(df=gck_train)\n",
    "    \n",
    "    Y_hat_df = nf.predict()\n",
    "    gck_predictions[gck] = Y_hat_df\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=val_product_dfs[gck].reset_index(), x='DATE', y='Sales_EUR', label='Actual Sales', color='#009999')\n",
    "    sns.lineplot(data=Y_hat_df, x='ds', y='NBEATS', label='NBEATS', marker='x', color='#2D373C', linestyle='dashed')\n",
    "    sns.lineplot(data=Y_hat_df, x='ds', y='NHITS', label='NHITS', marker='x', color='#E3700F',  linestyle='dashed')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title(f'Predicted Time Series for GCK {gck}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbeats_rmse = {}\n",
    "nhits_rmse = {}\n",
    "\n",
    "for gck in unique_gcks:\n",
    "    actual_sales = val_product_dfs[gck]['Sales_EUR']\n",
    "    nbeats_sales = gck_predictions[gck]['NBEATS'][:len(actual_sales)] \n",
    "    nhits_sales = gck_predictions[gck]['NHITS'][:len(actual_sales)] \n",
    "    \n",
    "    nbeats_rmse[gck] = root_mean_squared_error(actual_sales, nbeats_sales)\n",
    "    nhits_rmse[gck] = root_mean_squared_error(actual_sales, nhits_sales)\n",
    "\n",
    "nbeats_rmse = {gck: rmse_value for gck, rmse_value in nbeats_rmse.items() if rmse_value is not None}\n",
    "nhits_rmse = {gck: rmse_value for gck, rmse_value in nhits_rmse.items() if rmse_value is not None}\n",
    "\n",
    "for i in nbeats_rmse:\n",
    "    print(f\"NBEATS RMSE for {i}: {nbeats_rmse[i]}\")\n",
    "for i in nhits_rmse:\n",
    "    print(f\"NHITS RMSE for {i}: {nhits_rmse[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the group implemented Neural Forecast (NBEATS & NHITS) with selected exogenous features for each product.\n",
    "\n",
    "Best Performing Models (Lowest RMSE):\n",
    "- Product #20 – NBEATS (RMSE: 2551) | NHITS (RMSE: 2644)\n",
    "- Product #9 – NBEATS (RMSE: 6755) | NHITS (RMSE: 7629)\n",
    "- Product #13 – NBEATS (RMSE: 16583) | NHITS (RMSE: 17185)\n",
    "- Product #36 – NBEATS (RMSE: 16152) | NHITS (RMSE: 17350)\n",
    "\n",
    "Moderate Performance:\n",
    "- Products such as #12 (RMSE: NBEATS 224116 | NHITS 195330), #6 (RMSE: NBEATS 242462 | NHITS 239003), #4 (RMSE: NBEATS 223699 | NHITS 215436), and #8 (RMSE: NBEATS 654237 | NHITS 659060) had mid-range RMSE values.\n",
    "\n",
    "Underperforming Models (Highest RMSE):\n",
    "- Product #1 – NBEATS (RMSE: 4068513) | NHITS (RMSE: 3596081)\n",
    "- Product #5 – NBEATS (RMSE: 4239618) | NHITS (RMSE: 4930875)\n",
    "- Product #3 – NBEATS (RMSE: 2528315) | NHITS (RMSE: 2914800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_forecasts = {}\n",
    "\n",
    "def get_weights(rmse_dict):\n",
    "    \"\"\"Compute inverse RMSE weights and normalize them to sum to 1.\"\"\"\n",
    "    inv_rmse = {k: 1 / v for k, v in rmse_dict.items() if v > 0} \n",
    "    total = sum(inv_rmse.values())\n",
    "    \n",
    "    if total == 0: \n",
    "        return {k: 1 / len(rmse_dict) for k in rmse_dict}\n",
    "    \n",
    "    return {k: v / total for k, v in inv_rmse.items()}\n",
    "\n",
    "arimax_weights = get_weights({gck: results[gck]['rmse'] for gck in results})\n",
    "xgb_weights = get_weights(rmse_scores)\n",
    "prophet_weights = get_weights({gck: prophet_results[gck]['rmse'] for gck in prophet_results})\n",
    "nbeats_weights = get_weights(nbeats_rmse)\n",
    "nhits_weights = get_weights(nhits_rmse)\n",
    "timegpt_weights = get_weights(timegpt_rmse)\n",
    "\n",
    "num_models = 5 \n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    arimax_pred = results[gck]['y_pred'].values[:len(y_val)] if gck in results else np.zeros_like(y_val)\n",
    "    xgb_pred = forecasts[gck].values().flatten()[:len(y_val)] if gck in forecasts else np.zeros_like(y_val)\n",
    "    prophet_pred = prophet_results[gck]['y_pred'].values[:len(y_val)] if gck in prophet_results else np.zeros_like(y_val)\n",
    "    timegpt_pred = np.zeros_like(y_val)\n",
    "    \n",
    "    if gck in gck_predictions:\n",
    "        nbeats_pred = gck_predictions[gck]['NBEATS'].values[:len(y_val)]\n",
    "        nhits_pred = gck_predictions[gck]['NHITS'].values[:len(y_val)]\n",
    "    else:\n",
    "        nbeats_pred = np.zeros_like(y_val)\n",
    "        nhits_pred = np.zeros_like(y_val)\n",
    "\n",
    "    weights = {\n",
    "        'arimax': arimax_weights.get(gck, 1 / num_models),\n",
    "        'xgb': xgb_weights.get(gck, 1 / num_models),\n",
    "        'prophet': prophet_weights.get(gck, 1 / num_models),\n",
    "        'timegpt': timegpt_weights.get(gck, 1 / num_models),\n",
    "        'nbeats': nbeats_weights.get(gck, 1 / num_models),\n",
    "        'nhits': nhits_weights.get(gck, 1 / num_models),\n",
    "    }\n",
    "\n",
    "    total_weight = sum(weights.values())\n",
    "    weights = {k: v / total_weight for k, v in weights.items()}\n",
    "\n",
    "    ensemble_pred = (\n",
    "        weights['arimax'] * arimax_pred +\n",
    "        weights['xgb'] * xgb_pred +\n",
    "        weights['prophet'] * prophet_pred +\n",
    "        weights['timegpt'] * timegpt_pred +\n",
    "        weights['nbeats'] * nbeats_pred +\n",
    "        weights['nhits'] * nhits_pred\n",
    "    )\n",
    "\n",
    "    ensemble_forecasts[gck] = ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_gcks = ensemble_forecasts.keys()\n",
    "\n",
    "for gck in unique_gcks:\n",
    "\tactual_data = val_product_dfs[gck]\n",
    "\tforecasted_data = ensemble_forecasts[gck]\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tsns.lineplot(x=actual_data['DATE'], y=actual_data['Sales_EUR'], label='Actual Sales', color='#009999')\n",
    "\tsns.lineplot(x=actual_data['DATE'], y=forecasted_data, label='Ensemble Forecast', color='#2D373C', linestyle='dashed')\n",
    "\tplt.title(f'Actual vs Ensemble Forecasted Sales for GCK {gck}')\n",
    "\tplt.xlabel('Date')\n",
    "\tplt.ylabel('Sales (EUR)')\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_rmse = {}\n",
    "for gck in unique_gcks:\n",
    "    actual_sales = val_product_dfs[gck]['Sales_EUR']\n",
    "    forecasted_sales = ensemble_forecasts[gck]\n",
    "    ensemble_rmse[gck] = np.sqrt(mean_squared_error(actual_sales, forecasted_sales))\n",
    "    print(f\"RMSE for GCK {gck}: {ensemble_rmse[gck]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the group implemented an ensemble approach combining multiple forecasting models to improve predictive accuracy.\n",
    "\n",
    "Best Performing Models (Lowest RMSE):\n",
    "- Product #20 – RMSE: 2261\n",
    "- Product #9 – RMSE: 6114\n",
    "- Product #13 – RMSE: 12853\n",
    "- Product #14 – RMSE: 12626\n",
    "- Product #36 – RMSE: 15403\n",
    "- Product #16 - RMSE: 76286\n",
    "\n",
    "Moderate Performance:\n",
    "- Products such as #12 (RMSE: 106679), #11 (RMSE: 849925), #6 (RMSE: 249059), #4 (RMSE: 77273), and #8 (RMSE: 611629) showed mid-range RMSE values.\n",
    "\n",
    "Underperforming Models (Highest RMSE):\n",
    "- Product #1 – RMSE: 5935530\n",
    "- Product #5 – RMSE: 4553506\n",
    "- Product #3 – RMSE: 3912663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "In this notebook, the group had several forecasting approaches, such as XGBoost, Prophet, TimeGPT, Neural Forecast (using both NBEATS and NHITS), and an Ensemble model across different product groups. The RMSE results made us conclude some points:\n",
    "\n",
    "Strong Predictive Performance for Lower-Volume Products:\n",
    "- Products such as #20, #9, #13, #14, #36 and #16 consistently had low RMSE values, regardless of the model used, suggesting that, for these products, the models predict future sales with high accuracy.\n",
    "\n",
    "Moderate Performance for Mid-Range Products:\n",
    "- Products like #12, #6, #4, #8 and #11 exhibited RMSE values in the mid-range. While the forecasts for these products are reasonably accurate, there is still room for improvement through further feature refinement or hyperparameter tuning.\n",
    "\n",
    "Underperforming Models for High-Volume/High-Variability Products:\n",
    "- Products #1, #3, and #5 produced very high RMSE values (ranging from approximately 2.8 million to over 5.9 million). These results suggest that the forecasting models struggle to capture the patterns of these product groups, which may be due to higher volatility, additional structural changes, or missing key predictive variables.\n",
    "\n",
    "The group also concluded that, as expected, the ensemble model was the best perfomring model even though it had some obvious flaws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exporting the data for delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARIMAX\n",
    "\n",
    "results = {}\n",
    "\n",
    "gck_exog_vars = {\n",
    "    '#1': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Energy', 'Production Index United States: Electrical equipment', 'Production Index France: Electrical equipment'],\n",
    "    '#11': ['World: Price of Metals & Minerals'],\n",
    "    '#12': ['Shipments Index Machinery & Electricals France', 'Production Index Machinery & Electricals Italy', 'Production Index United States: Electrical equipment'],\n",
    "    '#13': None,\n",
    "    '#14': ['Production Index Machinery & Electricals Switzerland', 'World: Price of Energy'],\n",
    "    '#20': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Copper', 'World: Price of Metals & Minerals', 'World: Price of Natural gas index', 'Production Index United Kingdom: Electrical equipment'],\n",
    "    '#3': ['Production Index Machinery & Electricals United States', 'World: Price of Copper'],\n",
    "    '#36': ['Production Index Machinery & Electricals United States'],\n",
    "    '#4': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Natural gas index', 'Production Index Germany: Machinery and equipment n.e.c.'],\n",
    "    '#5': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'Producer Prices Germany: Electrical equipment', 'Production Index United States: Electrical equipment'],\n",
    "    '#6': ['Production Index Machinery & Electricals China', 'World: Price of Copper', 'World: Price of Energy'],\n",
    "    '#8': ['Production Index Machinery & Electricals China', 'Shipments Index Machinery & Electricals Italy', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Copper', 'Production Index Germany: Machinery and equipment n.e.c.', 'Production Index United States: Electrical equipment'],\n",
    "    '#9': ['World: Price of Copper'],\n",
    "    '#16' : None,\n",
    "}\n",
    "\n",
    "for gck in train_for_export.keys():\n",
    "    print(f\"Training ARIMAX for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_for_export[gck]\n",
    "    val_gck = val_for_export[gck]\n",
    "\n",
    "    train_gck = train_gck.drop(columns=['Mapped_GCK'])\n",
    "    val_gck = val_gck.drop(columns=['Mapped_GCK'])\n",
    "\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    y_val = val_gck['Sales_EUR']\n",
    "    if gck_exog_vars[gck] is not None:\n",
    "        X_train = train_gck[gck_exog_vars[gck]]\n",
    "        X_val = val_gck[gck_exog_vars[gck]]\n",
    "\n",
    "        model = SARIMAX(y_train, exog=X_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "        model_fit = model.fit(disp=False)\n",
    "\n",
    "        future_exog = pd.concat([X_train, X_val]).iloc[-12:]\n",
    "        y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + 11, exog=future_exog)\n",
    "    else:\n",
    "        model = ARIMA(y_train, order=(1, 1, 1))\n",
    "        model_fit = model.fit()\n",
    "        y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + 11)\n",
    "\n",
    "    y_pred = model_fit.predict(start=len(y_train), end=len(y_train) + len(y_val) - 1, exog=X_val)\n",
    "\n",
    "    y_pred.index = val_gck.index\n",
    "    y_pred_rmse = y_pred[val_gck['DATE'] <= '2022-04']\n",
    "    mse = mean_squared_error(y_val, y_pred_rmse)\n",
    "    rmse = np.sqrt(mse)\n",
    "    results[gck] = {'rmse': rmse, 'y_val': y_val, 'y_pred': y_pred_rmse}\n",
    "\n",
    "sarimax_rmse = {gck: result['rmse'] for gck, result in results.items() if 'rmse' in result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_export['#1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "\n",
    "for product in train_product_dfs.keys():\n",
    "    train_product_dfs[product] = train_product_dfs[product].reset_index()\n",
    "    val_product_dfs[product] = val_product_dfs[product].reset_index()\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import XGBModel\n",
    "from darts.metrics import rmse\n",
    "from darts.models.forecasting.xgboost import XGBModel\n",
    "\n",
    "forecasts = {}\n",
    "rmse_scores = {}\n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    print(f\"Training XGBoost for Product: {gck}\")\n",
    "\n",
    "    train_gck = train_for_export[gck].copy()\n",
    "    val_gck = val_for_export[gck].copy()\n",
    "\n",
    "    train_gck = train_gck.drop(columns=['Mapped_GCK'])\n",
    "    val_gck = val_gck.drop(columns=['Mapped_GCK'])\n",
    "\n",
    "    y_train = train_gck['Sales_EUR']\n",
    "    y_val = val_gck['Sales_EUR']\n",
    "\n",
    "    target_train = TimeSeries.from_dataframe(train_gck, time_col='DATE', value_cols=['Sales_EUR'])\n",
    "    target_val = TimeSeries.from_dataframe(val_gck, time_col='DATE', value_cols=['Sales_EUR'])\n",
    "\n",
    "    if gck in selected_features:\n",
    "        X_train = train_gck[selected_features[gck]]\n",
    "        X_val = val_gck[selected_features[gck]]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        exog_train = TimeSeries.from_dataframe(train_gck, time_col='DATE', value_cols=selected_features[gck])\n",
    "        exog_val = TimeSeries.from_dataframe(val_gck, time_col='DATE', value_cols=selected_features[gck])\n",
    "    else:\n",
    "        exog_train, exog_val = None, None  \n",
    "\n",
    "\n",
    "    model = XGBModel(lags=9, lags_exog=9 if exog_train else None)\n",
    "    model.fit(target_train, past_covariates=exog_train)\n",
    "\n",
    "    y_pred = model.predict(n=12, past_covariates=exog_val)\n",
    "\n",
    "    forecasts[gck] = y_pred\n",
    "\n",
    "    y_pred.index = val_gck.index\n",
    "    y_pred_rmse = y_pred[val_gck['DATE'] <= '2022-04']\n",
    "    rmse_value = rmse(target_val, y_pred)\n",
    "    rmse_scores[gck] = rmse_value\n",
    "\n",
    "xgb_rmse = {gck: rmse_value for gck, rmse_value in rmse_scores.items() if rmse_value is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "\n",
    "prophet_results = {}\n",
    "\n",
    "for product in mapped_gck_list:\n",
    "    print(f\"Processing Product: {product}\")\n",
    "    \n",
    "    df_train = train_product_dfs[product].reset_index()\n",
    "    df_val   = val_product_dfs[product].reset_index()\n",
    "    \n",
    "    df_prophet = df_train[['DATE', 'Sales_EUR']].rename(columns={'DATE': 'ds', 'Sales_EUR': 'y'})\n",
    "    \n",
    "    model = Prophet(seasonality_mode='multiplicative', yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "    model.fit(df_prophet)\n",
    "    \n",
    "    forecast_periods = len(df_val)\n",
    "    future = model.make_future_dataframe(periods=forecast_periods, freq='MS')\n",
    "    \n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    y_pred = forecast['yhat'][-forecast_periods:]\n",
    "\n",
    "    mse = mean_squared_error(df_val['Sales_EUR'], y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    prophet_results[product] = {'rmse': rmse, 'y_val': df_val['Sales_EUR'], 'y_pred': y_pred}\n",
    "\n",
    "    print(f\"Product: {product}, RMSE: {rmse}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_val['DATE'], df_val['Sales_EUR'], label='Actual Sales', color = '#009999')\n",
    "    plt.plot(df_val['DATE'], y_pred, label='Predicted Sales', linestyle='dashed', color='#2D373C')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales (EUR)')\n",
    "    plt.title(f'Prophet Model - Actual vs Predicted Sales for {product}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for product, result in prophet_results.items():\n",
    "    print(f\"Product: {product}, RMSE: {result['rmse']}\")\n",
    "\n",
    "prophet_rmse = {gck: rmse_value for gck, rmse_value in prophet_results.items() if rmse_value is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeGPT\n",
    "\n",
    "nixtla_client = NixtlaClient(\n",
    "    api_key = 'nixak-IJDy3xU0F51O6p8oD2ZxTm1f0qOuwPofNu8bXP1NpIpGj8O5uVN8WE3YecfIJh0DLMi2AMof6672XL4t')\n",
    "\n",
    "nixtla_client.validate_api_key()\n",
    "\n",
    "trans_train['DATE'] = pd.to_datetime(trans_train['DATE'].dt.to_timestamp())\n",
    "trans_val['DATE'] = pd.to_datetime(trans_val['DATE'].dt.to_timestamp())\n",
    "\n",
    "timegpt_fcst_df = nixtla_client.forecast(trans_train, time_col='DATE', target_col='Sales_EUR', id_col='Mapped_GCK', h = 9, freq='MS', model='timegpt-1-long-horizon')\n",
    "timegpt_fcst_df.head()\n",
    "\n",
    "unique_gcks = timegpt_fcst_df['Mapped_GCK'].unique()\n",
    "\n",
    "timegpt_rmse = {}\n",
    "for gck in unique_gcks:\n",
    "    actual_sales = trans_val[trans_val['Mapped_GCK'] == gck]['Sales_EUR']\n",
    "    forecasted_sales = timegpt_fcst_df[timegpt_fcst_df['Mapped_GCK'] == gck]['TimeGPT']\n",
    "    timegpt_rmse[gck] = root_mean_squared_error(actual_sales, forecasted_sales)\n",
    "\n",
    "timegpt_rmse = {gck: rmse_value for gck, rmse_value in timegpt_rmse.items() if rmse_value is not None}\n",
    "for i in timegpt_rmse:\n",
    "    print(i, timegpt_rmse[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Forecast\n",
    "\n",
    "gck_features = {\n",
    "    '#1': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Energy', 'Production Index United States: Electrical equipment', 'Production Index France: Electrical equipment'],\n",
    "    '#11': ['World: Price of Metals & Minerals'],\n",
    "    '#12': ['Shipments Index Machinery & Electricals France', 'Production Index Machinery & Electricals Italy', 'Production Index United States: Electrical equipment'],\n",
    "    '#13': None,\n",
    "    '#14': ['Production Index Machinery & Electricals Switzerland', 'World: Price of Energy'],\n",
    "    '#20': ['Production Index Machinery & Electricals China', 'Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Copper', 'World: Price of Metals & Minerals', 'World: Price of Natural gas index', 'Production Index United Kingdom: Electrical equipment'],\n",
    "    '#3': ['Production Index Machinery & Electricals United States', 'World: Price of Copper'],\n",
    "    '#36': ['Production Index Machinery & Electricals United States'],\n",
    "    '#4': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'World: Price of Natural gas index', 'Production Index Germany: Machinery and equipment n.e.c.'],\n",
    "    '#5': ['Production Index Machinery & Electricals Germany', 'Production Index Machinery & Electricals Japan', 'Producer Prices Germany: Electrical equipment', 'Production Index United States: Electrical equipment'],\n",
    "    '#6': ['Production Index Machinery & Electricals China', 'World: Price of Copper', 'World: Price of Energy'],\n",
    "    '#8': ['Production Index Machinery & Electricals China', 'Shipments Index Machinery & Electricals Italy', 'Production Index Machinery & Electricals Switzerland', 'World: Price of Copper', 'Production Index Germany: Machinery and equipment n.e.c.', 'Production Index United States: Electrical equipment'],\n",
    "    '#9': ['World: Price of Copper'],\n",
    "    '#16' : None\n",
    "    }\n",
    "\n",
    "gck_predictions = {}\n",
    "\n",
    "for gck, features in gck_features.items():\n",
    "    print(f\"Processing GCK: {gck}\")\n",
    "    \n",
    "    gck_train = train_product_dfs[gck].reset_index()\n",
    "    gck_train = gck_train.rename(columns={'DATE': 'ds', 'Sales_EUR': 'y', 'Mapped_GCK': 'unique_id'})\n",
    "    gck_train = gck_train[['ds', 'y', 'unique_id'] + (features if features else [])]\n",
    "    \n",
    "    models = [\n",
    "        NBEATS(input_size=2 * 10, h=10, max_steps=100, enable_progress_bar=False),\n",
    "        NHITS(input_size=2 * 10, h=10, max_steps=100, enable_progress_bar=False)\n",
    "    ]\n",
    "    nf = NeuralForecast(models=models, freq='MS')\n",
    "    \n",
    "    nf.fit(df=gck_train)\n",
    "    \n",
    "    Y_hat_df = nf.predict()\n",
    "    gck_predictions[gck] = Y_hat_df\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=val_product_dfs[gck].reset_index(), x='DATE', y='Sales_EUR', label='Actual Sales', color='#009999')\n",
    "    sns.lineplot(data=Y_hat_df, x='ds', y='NBEATS', label='NBEATS', marker='x', color='#2D373C', linestyle='dashed')\n",
    "    sns.lineplot(data=Y_hat_df, x='ds', y='NHITS', label='NHITS', marker='x', color='#E3700F',  linestyle='dashed')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title(f'Predicted Time Series for GCK {gck}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_forecasts = {}\n",
    "\n",
    "def get_weights(rmse_dict):\n",
    "    \"\"\"Compute inverse RMSE weights and normalize them to sum to 1.\"\"\"\n",
    "    inv_rmse = {k: 1 / v for k, v in rmse_dict.items() if v > 0} \n",
    "    total = sum(inv_rmse.values())\n",
    "    \n",
    "    if total == 0: \n",
    "        return {k: 1 / len(rmse_dict) for k in rmse_dict}\n",
    "    \n",
    "    return {k: v / total for k, v in inv_rmse.items()}\n",
    "\n",
    "arimax_weights = get_weights({gck: results[gck]['rmse'] for gck in results})\n",
    "xgb_weights = get_weights(rmse_scores)\n",
    "prophet_weights = get_weights({gck: prophet_results[gck]['rmse'] for gck in prophet_results})\n",
    "nbeats_weights = get_weights(nbeats_rmse)\n",
    "nhits_weights = get_weights(nhits_rmse)\n",
    "timegpt_weights = get_weights(timegpt_rmse)\n",
    "\n",
    "num_models = 5 \n",
    "\n",
    "for gck in train_product_dfs.keys():\n",
    "    arimax_pred = results[gck]['y_pred'].values[:len(y_val)] if gck in results else np.zeros_like(y_val)\n",
    "    xgb_pred = forecasts[gck].values().flatten()[:len(y_val)] if gck in forecasts else np.zeros_like(y_val)\n",
    "    prophet_pred = prophet_results[gck]['y_pred'].values[:len(y_val)] if gck in prophet_results else np.zeros_like(y_val)\n",
    "    timegpt_pred = np.zeros_like(y_val)\n",
    "\n",
    "    if gck in gck_predictions:\n",
    "        nbeats_pred = gck_predictions[gck]['NBEATS'].values[:len(y_val)]\n",
    "        nhits_pred = gck_predictions[gck]['NHITS'].values[:len(y_val)]\n",
    "    else:\n",
    "        nbeats_pred = np.zeros_like(y_val)\n",
    "        nhits_pred = np.zeros_like(y_val)\n",
    "\n",
    "    weights = {\n",
    "        'arimax': arimax_weights.get(gck, 1 / num_models),\n",
    "        'xgb': xgb_weights.get(gck, 1 / num_models),\n",
    "        'prophet': prophet_weights.get(gck, 1 / num_models),\n",
    "        'timegpt': timegpt_weights.get(gck, 1 / num_models),\n",
    "        'nbeats': nbeats_weights.get(gck, 1 / num_models),\n",
    "        'nhits': nhits_weights.get(gck, 1 / num_models),\n",
    "    }\n",
    "\n",
    "    total_weight = sum(weights.values())\n",
    "    weights = {k: v / total_weight for k, v in weights.items()}\n",
    "\n",
    "    ensemble_pred = (\n",
    "        weights['arimax'] * arimax_pred +\n",
    "        weights['xgb'] * xgb_pred +\n",
    "        weights['prophet'] * prophet_pred +\n",
    "        weights['timegpt'] * timegpt_pred +\n",
    "        weights['nbeats'] * nbeats_pred +\n",
    "        weights['nhits'] * nhits_pred\n",
    "    )\n",
    "\n",
    "    ensemble_forecasts[gck] = ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for export\n",
    "ensemble_predictions = []\n",
    "\n",
    "for gck, predictions in ensemble_forecasts.items():\n",
    "    for date, sales in zip(val_product_dfs[gck]['DATE'], predictions):\n",
    "        ensemble_predictions.append({'DATE': date, 'Mapped_GCK': gck, 'Sales_EUR': sales})\n",
    "\n",
    "# Convert to DataFrame\n",
    "ensemble_predictions_df = pd.DataFrame(ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "ensemble_predictions_df.to_csv('ensemble_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. References\n",
    "<sup>1</sup> Pallavi Padav. (2024, November). Granger causality in time series explained using chicken and egg problem. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/08/granger-causality-in-time-series-explained-using-chicken-and-egg-problem/\n",
    "\n",
    "<sup>2</sup> Adam Hayes. (2024, July). Autoregressive integrated moving average (ARIMA). Retrieved March 27, 2025, from https://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp\n",
    "\n",
    "<sup>3</sup> GeeksforGeeks. (n.d.). Complete guide to SARIMAX in Python. GeeksforGeeks. Retrieved March 29, 2025, from https://www.geeksforgeeks.org/complete-guide-to-sarimax-in-python\n",
    "\n",
    "<sup>4</sup> Github. (n.d. ) Prophet: Automatic Forecasting Procedure. Github. Retrieved March 29, 2025, from https://github.com/facebook/prophet\n",
    "\n",
    "<sup>5</sup> NIXTLA. (n.d. ) About TimeGPT. NIXTLA. Retrieve March 29, 2025, from https://docs.nixtla.io/docs/getting-started-about_timegpt\n",
    "\n",
    "<sup>6</sup> Github. (n.d. ) Neural Forecast, User friendly state-of-the-art neural forecasting models. Github. Retrieve March 29, 2025, from https://github.com/Nixtla/neuralforecast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
